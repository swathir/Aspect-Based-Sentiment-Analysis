{
 "metadata": {
  "name": "",
  "signature": "sha256:075c02a4df0d5dd9ca2ad9d06bde7e41a7bffdaa136ffc8123a4347f10b6e9f6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "tree = ET.parse('WeatherForecast.asmx')\n",
      "root = tree.getroot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' for twenty five neg files '''\n",
      "\n",
      "import nltk\n",
      "from nltk.tokenize import *\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "from operator import itemgetter\n",
      "import re\n",
      "from nltk.tag.stanford import POSTagger,NERTagger\n",
      "import os\n",
      "from textblob import TextBlob\n",
      "\n",
      "from textblob.sentiments import NaiveBayesAnalyzer\n",
      "\n",
      "java_path = \"/opt/jdk/jdk1.8.0_05/bin/java.exe\" # replace this\n",
      "os.environ['JAVAHOME'] = java_path\n",
      "os.environ['CLASSPATH'] = '/home/swathi/stanford-ner-2015-04-20/stanford-ner.jar';\n",
      "'/home/swathi/stanford-postagger-2015-04-20/stanford-pos.jar'\n",
      "os.environ['STANFORD_MODELS'] = '/home/swathi/stanford-ner-2015-04-20/stanford-ner.jar';\n",
      "'/home/swathi/stanford-postagger-2015-04-20/models'\n",
      "\n",
      "st = NERTagger(\"/home/swathi/stanford-ner-2015-04-20/classifiers/english.all.3class.distsim.crf.ser.gz\",\n",
      "               \"/home/swathi/stanford-ner-2015-04-20/stanford-ner.jar\")\n",
      "english_postagger = POSTagger('/home/swathi/stanford-postagger-2015-04-20/models/english-bidirectional-distsim.tagger',\n",
      "                              '/home/swathi/stanford-postagger-2015-04-20/stanford-postagger.jar')\n",
      "categories_m = {'plot':['plot', 'conflict', 'theme',\n",
      "                            'point of view', 'mood', 'symbolism','ending','story',\n",
      "                            'script', 'tension', 'pacing', 'dialogue','storyline','plots'],\n",
      "              'actors':['acting', 'costumes', 'make-up','performance','actors','actress', 'actor',\n",
      "                        'part','parts', 'cast','casts','actresses','villian','villains','elements','element',\n",
      "                        'ones','performances','veterans','veteran', 'characters','character'],\n",
      "              'cinematic':['cinematography','camera angles', 'composition', 'lighting',\n",
      "                           'lights','camera', 'setting','settings',\n",
      "                            'editing', 'special effects','sets'],\n",
      "                'sound':['music','sound efects','sound','sounds','music score','background music'\n",
      "                         ]\n",
      "                }\n",
      "files=[]\n",
      "positivity=negativity=neutral=0\n",
      "Abs_noun = ['brilliant', 'nice','awesome']\n",
      "with open('/home/swathi/DATA_FOURSQUARE/abstract_nouns', 'r') as f:\n",
      "        for line in f:\n",
      "            data=\"'\"+line[0:].strip()+\"'\"+\",\"\n",
      "            main=Abs_noun.append(line.strip('\\n\\t'))\n",
      "\n",
      "for filename in os.listdir(\"/home/swathi/nltk_data/corpora/movie_reviews/temp2\"):\n",
      "    files.append(filename)\n",
      "#print len(files)\n",
      "\n",
      "for revfile in files:\n",
      "    text = \"\"\n",
      "    #print revfile\n",
      "    with open(\"/home/swathi/nltk_data/corpora/movie_reviews/temp2/\"+str(revfile)) as f:\n",
      "        text = f.read()\n",
      "    final=0\n",
      "    text = text.replace(\"\\\"\",\"\\'\")\n",
      "    text = text.replace(\"n't\",\" not\")\n",
      "    text = re.split(\"\\n\",text)\n",
      "\n",
      "    '''\n",
      "    text = \"The food is great, but service is really bad . try the mangoes . \\\n",
      "    have papaya here .\\\n",
      "    these are great\"'''\n",
      "        #print text\n",
      "    reviews=[]\n",
      "    reviews=text\n",
      "        #print \"sentences is \",len(reviews)\n",
      "    f.close()\n",
      "    adj=[]\n",
      "    noun=[]\n",
      "    newlist=[]\n",
      "    nearby=''\n",
      "    dummy=0.0\n",
      "    dum=0\n",
      "    #print reviews \n",
      "    #str1=''.join(text) #string of text list\n",
      "    flag_p=flag_s=flag_a=flag_c=flag_m=0\n",
      "    tot_p=tot_a=tot_c=tot_s=tot_m=0\n",
      "\n",
      "    for review in reviews:\n",
      "        s1 = str(review)\n",
      "        #print s1\n",
      "        #st.tag(review)\n",
      "        tkn = wordpunct_tokenize(s1)\n",
      "        #print tkn\n",
      "\n",
      "        #initialise their ints for each review\n",
      "        plot_int=0\n",
      "        act_int=0\n",
      "        cinem_int=0\n",
      "        sound_int=0\n",
      "        misc_int=0\n",
      "\n",
      "        #datan=st.tag(nltk.word_tokenize(s1))\n",
      "        datap=english_postagger.tag(s1.split())\n",
      "        #print datap\n",
      "        for item in datap:\n",
      "            #print item\n",
      "            for words in item:\n",
      "                if words == 'NN' or words == 'NNR' or words == 'NNS':\n",
      "                    noun.append(item[0])#add to noun\n",
      "                    #print noun\n",
      "                    #find nearest adj and assign nearby\n",
      "                    non_word=item[0]\n",
      "                    dum=map(itemgetter(0),datap).index(str(item[0]))\n",
      "                    #print dum\n",
      "                    #grammar = r\"\"\"NP: {<JJ|JJR|JJS>*<DT|PP\\$>?<NN|NNR|NNS>?<NN|NNR|NNS>}\"\"\"  \n",
      "                    #cp = nltk.RegexpParser(grammar)\n",
      "                    #result = cp.parse(datap)\n",
      "                    #print(result)\n",
      "                    if dum>1:\n",
      "                        word=datap[dum-1]\n",
      "                        #print word\n",
      "                        if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':\n",
      "                            adj.append(word[0])\n",
      "                            near=word[0]\n",
      "                            string=non_word+' '+near\n",
      "                            blob = TextBlob(string, analyzer=NaiveBayesAnalyzer())\n",
      "                            for nearby in blob.sentences:\n",
      "                                dummy=nearby.sentiment.p_pos\n",
      "                                if dummy > 0.50 :\n",
      "                                    dummy=dummy\n",
      "                                else :\n",
      "                                    dummy=-1*(float(1-dummy))\n",
      "                            newlist.append((non_word,near,dummy))\n",
      "                    if dum>2:\n",
      "                        word=datap[dum-2]\n",
      "                        #print word\n",
      "                        if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':\n",
      "                            adj.append(word[0])\n",
      "                            near=word[0]\n",
      "                            string=non_word+' '+near\n",
      "                            blob = TextBlob(string, analyzer=NaiveBayesAnalyzer())\n",
      "                            for nearby in blob.sentences:\n",
      "                                dummy=nearby.sentiment.p_pos\n",
      "                                if dummy > 0.50 :\n",
      "                                    dummy=dummy\n",
      "                                else :\n",
      "                                    dummy=-1*(float(1-dummy))\n",
      "                            newlist.append((non_word,near,dummy))\n",
      "                    if dum>3:\n",
      "                        word=datap[dum-3]\n",
      "                        if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':\n",
      "                            adj.append(word[0])\n",
      "                            near=word[0]\n",
      "                            string=non_word+' '+near\n",
      "                            blob = TextBlob(string, analyzer=NaiveBayesAnalyzer())\n",
      "                            for nearby in blob.sentences:\n",
      "                                dummy=nearby.sentiment.p_pos\n",
      "                                if dummy > 0.50 :\n",
      "                                    dummy=dummy\n",
      "                                else :\n",
      "                                    dummy=-1*(float(1-dummy))\n",
      "                            newlist.append((non_word,near,dummy))\n",
      "\n",
      "                    '''\n",
      "                    if words == 'JJ' or words == 'JJR' or words == 'JJS':\n",
      "                    adj.append(item[0])\n",
      "                    near=item[0]\n",
      "                    '''\n",
      "\n",
      "    print newlist\n",
      "                    #near=\"excellent\"\n",
      "                    #blob = TextBlob(''.join(near))\n",
      "                    #for nearby in blob.sentences:\n",
      "                    #   dummy=nearby.sentiment.polarity\n",
      "\n",
      "                    # for words in nouns:\n",
      "                        #words.append(adjective)\n",
      "                    #link to noun\n",
      "    #traverse through the newlist and assign to eacg int and divide by flag\n",
      "    if newlist:\n",
      "        for x in newlist:\n",
      "            if str(x[0]) in categories_m['plot']:\n",
      "                plot_int=plot_int+float(str(x[2]).strip(\"[\"']'))\n",
      "                flag_p=flag_p+1\n",
      "            elif str(x[0]) in categories_m['actors']:\n",
      "                act_int=act_int+float(str(x[2]).strip(\"[\"']'))\n",
      "                flag_a=flag_a+1\n",
      "            elif str(x[0]) in categories_m['cinematic']:\n",
      "                cinem_int=cinem_int+float(str(x[2]).strip(\"[\"']'))\n",
      "                flag_c=flag_c+1\n",
      "            elif str(x[0]) in categories_m['sound']:\n",
      "                sound_int=sound_int+float(str(x[2]).strip(\"[\"']'))\n",
      "                flag_s=flag_s+1\n",
      "            else :\n",
      "                misc_int=misc_int+float(str(x[2]).strip(\"[\"']'))\n",
      "                flag_m=flag_m+1\n",
      "\n",
      "    if flag_p!=0:\n",
      "        tot_p=tot_p+float(float(plot_int)/flag_p)\n",
      "    if flag_a!=0:\n",
      "        tot_a=tot_a+float(float(act_int)/flag_a)\n",
      "    if flag_c!=0:\n",
      "        tot_c=tot_c+float(float(cinem_int)/flag_c)\n",
      "    if flag_s!=0:\n",
      "        tot_s=tot_s+float(float(sound_int)/flag_s)\n",
      "    if flag_m!=0:\n",
      "        tot_m=tot_m+float(float(misc_int)/flag_m)\n",
      "#print tot_p\n",
      "#print tot_a\n",
      "#print tot_c\n",
      "#print tot_s\n",
      "#print tot_m\n",
      "    final=float((tot_p+tot_a+tot_c+tot_s+tot_m)/5)\n",
      "    #print final\n",
      "    if final<0 :\n",
      "        negativity= negativity +1\n",
      "        \n",
      "print float(float(negativity)/10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'thrillers', u'erotic', -0.7479508196721312), (u'1990s', u'early', -0.5504614176320444), (u'woman', u'early', -0.538730860201668), (u'decade', u'previous', 0.6616094986807389), (u'1980s', u'nostalgic', 0.5526315789473684), (u'reason', u'good', -0.6144599819398977), (u'jonathan', u'powerful', 0.7049878345498782), (u'jonathan', u'rich', 0.6388647710006515), (u'night', u'stormy', 0.6646645180193379), (u'friend', u'good', 0.5311111289365185), (u'walter', u'good', -0.558335096255553), (u'j', u'good', 0.5042265426880812), (u'lawyer', u'good', 0.6079330637472466), (u'genre', u'more', 0.5705597286164285), (u'thriller', u'erotic', -0.6899641577060929), (u'woman', u'erotic', -0.637287024901704), (u'films', u'similar', 0.6643464891959058), (u'idiots', u'total', -0.8668952618453865), (u'hand', u'other', 0.5504469064519079), (u'compensation', u'small', 0.6632797731569), (u'lack', u'apparent', -0.582039911308204), (u'hand', u'other', 0.5504469064519079), (u'flashbacks', u'many', 0.6531690529241766), (u'angles', u'strange', -0.5514360803454104), (u'shooting', u'strange', 0.568229843953186), (u'viewer', u'dissatisfied', -0.5375939849624058), (u'viewer', u'bored', -0.5341985673573129)]\n",
        "[(u'rienfenstal', u'leni', -0.9000000000000002), (u'piece', u'unenjoyable', 0.518604651162791), (u'piece', u'hateful', -0.6368078175895768), (u'pornography', u'violent', -0.6227722772277224), (u'pornography', u'violent', -0.6227722772277224), (u'cinema', u'good', 0.6020965366604338), (u'storytelling', u'good', 0.6737272420768711), (u'fun', u'stupid', -0.7738634237605237), (u\"it's\", u'stupid', -0.7974358974358975), (u'members', u'least', -0.5855025890739673), (u'movies', u'worst', -0.813395427700081), (u'time', u'long', 0.5087479364257274), (u'anything', u'abstract', 0.7501881821603316), (u'metal', u'full', -0.5688293715402148), (u'jacket', u'full', -0.5863402061855669), (u'moment', u'climact', -0.7541743970315401), (u'joker', u'private', 0.5290697674418605), (u'eternity', u'near-pornographic', -0.5416666666666666), (u'recruitment', u'giant', -0.5530973451327432), (u'film', u'giant', -0.5415128218097212), (u'species', u'alien', -0.5648836841272351), (u'thing', u'stupidest', -0.8342457420924573), (u'thing', u'single', -0.539674042350702), (u'wwii', u'bad', 0.6020881670533642), (u'movie', u'bad', -0.6861682816141298), (u'points', u'obvious', -0.5890416509102694), (u'satire', u'obvious', -0.564743911624404), (u'soldiers', u'filimg', -0.6758241758241763), (u'intelligence', u'tactical', -0.85), (u'sense', u'common', 0.6352432714291445), (u'*', u'bad', -0.6647286821705427), (u'movie', u'bad', -0.6861682816141298), (u'*', u'bad', -0.6647286821705427), (u'anything', u\"there's\", -0.5499328458040966), (u'appelation', u'better', -0.7840195394277741), (u'paul', u'talent-impaired', -0.5591715976331362), (u'situations', u'tight', 0.5538838812301166), (u'time', u'sixth', 0.643410734909732), (u'time', u'fifth', -0.6129449775199984), (u\"it's\", u'sixth', 0.6346153846153844), (u'bugs', u'effective', 0.6633582438113028), (u'auto', u'full', -0.6970658888126287), (u'platoons', u'whole', -0.533078880407125), (u'bugs', u'whole', -0.6061129258049464), (u'feature', u'worst', -0.7886353104726599), (u'flavor', u'quasi-fascist', -0.5937500000000001), (u'flavor', u'repulsive', -0.6411042944785279), (u'trappings', u'many', -0.5810852237890867), (u'subject', u'intelligent', 0.7367082656637319), (u'comparison', u'innocuous', -0.5959409594095944), (u'arsenic', u'bland', -0.9268867924528302), (u'camerawork', u'disposable', -0.5557692307692311), (u'thing', u'heartbreaking', 0.738830602381233)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'saving', u'worth', 0.528268551236749), (u'tv', u'more', 0.5112244897959184), (u'market', u'international', 0.6407055093143078), (u'piece', u'serious', -0.5055989779047045), (u'cinema', u'serious', 0.574553001277139), (u'spoilers', u'minor', 0.6868221258134488), (u'hospitality', u'extraordinary', 0.6488764044943819), (u'yucks', u'big', -0.7743722304283606), (u'part', u'large', 0.5801218638862928), (u'satire', u'large', 0.5863357215967249), (u'produce', u'poor', -0.6805555555555557), (u'tomato', u'good', -0.8309719467271182), (u'grocery', u'good', 0.6111680327868856), (u'film', u'short', 0.5467957812542819), (u'film', u'20-minute', 0.5116875712656785)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'man', u'unstable', 0.5628238341968915), (u'psychotherapy', u'unstable', -0.8152173913043478), (u'accident', u'fatal', -0.5304927302100164), (u'restauranteur', u'fledgling', -0.7941176470588238), (u'string', u'endless', -0.5633977216443782), (u'type', u'spurned-psychos-getting-their-revenge', 0.5055865921787708), (u'movies', u'spurned-psychos-getting-their-revenge', 0.5006793478260869), (u'category', u'stable', -0.5715686274509799), (u'part', u'due', 0.622420767261226), (u'(', u'inexpensive', 0.6249999999999998), (u'effects', u'special', 0.5133333333333334), (u'name', u'big', -0.5843651655283304), (u'stars', u'big', -0.5394562170091937), (u'(', u'inexpensive', 0.6249999999999998), (u'cable', u'late-night', -0.621951219512195), (u'cable', u'frequent', 0.5333089311859446), (u'television', u'late-night', -0.5), (u'television', u'frequent', 0.652777777777778), (u')', u'big', -0.533587786259542), (u')', u'ex-husband', -0.5), (u'entry', u'redundant', -0.5603448275862066), (u'suspense', u'much', -0.5723742820302427), (u'narrator', u'serious-sounding', 0.6166666666666667), (u'statistics', u'serious-sounding', -0.7500000000000003), (u'pictures', u')', 0.5707547169811319), (u'bit', u'contrived', -0.5916872427983542), (u'cliche', u'psycho-in-love', -0.6818181818181817), (u'pet', u'dead', -0.5940366972477067), (u'showdown', u'inevitable', 0.6155778894472368), (u'stalker', u'inevitable', 0.5188679245283015), (u'anything', u'adequate', -0.5171406310868719), (u'words', u'other', 0.5534492983748315), (u'actor', u'only', -0.51240780626708), (u'times', u'ditzy', 0.5866117244596488), (u'business-owner', u'independent', 0.6627906976744184), (u'business-owner', u'strong', 0.6599999999999999), (u'door', u'front', -0.5679137860919077), (u'daryl', u'front', 0.5922519509476031), (u'presence', u'oblivious', 0.6398561695685084), (u'episode', u'whole', -0.5271478809191779), (u'strain', u'incredible', 0.6938868613138687), (u'suspension', u\"audience's\", -0.6826146788990823), (u'disbelief', u\"audience's\", -0.5302892321142356), (u'murder', u'several', 0.5885396233817185), (u'scenes', u'several', 0.5739115954844562), (u'nudity', u'brief', -0.5849555240428002), (u'thrillers', u'other', -0.6176121509567113), (u'thrillers', u'many', -0.5666334060426044), (u'suspense', u'good', -0.5725341383591016), (u'film', u'good', 0.5159109700721598)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'man', u'black', 0.5501622698104394), (u'girl', u'white', 0.5304189993002101), (u'cop', u'black', -0.5685964259121369), (u'cop', u'tough', -0.5234054310072627), (u'rights', u'human', 0.6676416994341614), (u'screen', u'remarkable', 0.7262445295404812), (u'debut', u'remarkable', 0.7361285266457678), (u'comedy', u'tragic', 0.613762249680443), (u'effect', u'fine', 0.6045994065281898), (u'film', u'stuffy', -0.6139784946236557), (u'dublin', u'lovable', -0.5100000000000011), (u'bus', u'lovable', 0.6824048913043479), (u'conductor', u'lovable', 0.8384030418250946), (u'magic', u'sweet', 0.7605421686746986), (u'finney', u'sweet', -0.5), (u'stiffs', u'working-class', -0.8333333333333335), (u'administrator', u'stuff--an', -0.7987012987012991), (u'administrator', u'small', -0.6807324840764335), (u'pork', u'catholic', -0.6590909090909086), (u'pork', u'outraged', -0.5434782608695652), (u'butcher', u'catholic', -0.5370370370370373), (u'butcher', u'outraged', 0.5833333333333333), (u'leland', u'gorgeous', 0.5377358490566038), (u'bus', u'double-decker', -0.5729166666666666), (u'bus', u'gorgeous', 0.5483341154387611), (u'alley', u'dark', -0.500753012048193), (u'ride', u'enjoyable', 0.7510030492697801), (u'picture', u'complete', 0.5220089324057372), (u'atmosphere', u'airy', 0.6088709677419355), (u'atmosphere', u'light', 0.7150821244455102), (u'misstep', u'fatal', -0.5967741935483873), (u'fun', u'most', 0.5791213166199258), (u'*', u'most', 0.5446503791069924), (u'month', u'last', 0.52646933560477), (u'comedy', u'pitch-black', -0.5326086956521741), (u'dr', u'pitch-black', -0.5), (u'pickens', u'slim', -0.6176470588235297), (u'film', u'younger', 0.5836274330305001)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'features', u'animated', 0.6183735860593094), (u\"kids'\", u'live-action', 0.5204545454545455), (u'entertainment', u'live-action', 0.5887096774193548), (u'occasions', u'double', -0.5441176470588236), (u'adaptation', u'live-action', 0.54), (u'cartoon', u'popular', -0.6055004508566275), (u'(', u'popular', 0.5335365853658537), (u'piece', u'expensive', -0.538375796178344), (u'trash', u'expensive', -0.6833888426311407), (u\"children's\", u'viable', 0.7460526315789474), (u'film', u'viable', 0.7097266154541345), (u'adults', u'unbearable', -0.841176470588235), (u'time', u'skinny', -0.6020118762740407), (u'blockbuster', u'local', 0.5245077720207256), (u'rent', u'local', -0.6866797900262466), (u'budgets', u'important', -0.6006686478454678), (u'effects', u'fancy', -0.606149012567325), (u'security', u'depressed', -0.6484446477584631), (u'guard', u'depressed', -0.5130597014925377), (u'heart', u'big', 0.5884597059624284), (u'policemen', u'real', -0.5934470377019747), (u'deeds', u'heroic', 0.861111111111111), (u'admiration', u'subsequent', 0.7000000000000001), (u'time', u'long', 0.5087479364257274), (u'crush', u'long', -0.520013261799988), (u'coincidences', u'bizarre', -0.7615384615384615), (u'coincidences', u'few', -0.7407541696881794), (u'super-policeman', u'half-machine', -0.5), (u'super-policeman', u'half-man', -0.5), (u'gadgets', u'nifty', 0.6028446389496717), (u'murder', u'responsible', 0.5369760355760518), (u\"doctor's\", u'good', -0.622472030457347), (u'father', u'good', 0.5972685550483915), (u'carbon', u'own', -0.767080745341615), (u'nemesis', u'worst', -0.8261128100577644), (u'(', u'mobile', 0.7083333333333333), (u'voice', u'mobile', 0.7714579055441474), (u'pace', u'mad', 0.5468079213405348), (u'atmosphere', u'convincing', 0.6733709501274118), (u'gizmos', u'fancy', -0.7115384615384615), (u'dollar', u'fancy', -0.6729419272495212), (u'effect', u'same', 0.546810747302353), (u'ingredients', u'raw', 0.7816901408450705), (u'f/x', u'expensive', -0.556818181818182), (u'amounts', u'expensive', -0.7376681614349778), (u'irreverence', u'sly', 0.8043478260869567), (u'movie', u'condescending', 0.5172998278088674), (u'bumper', u'back', -0.6049361408353469), (u'vehicle', u'moving', -0.5454285714285715), (u'complaints', u'complex', 0.9075615050651227), (u'stuff', u'simple', 0.5586788042809695), (u'inspector', u'simple', 0.7181372549019603), (u'film', u'other', 0.5359928622779091), (u\"i've\", u'other', 0.5243462578899911), (u'(', u'commercial', -0.5204081632653061), (u'(', u'popular', 0.5335365853658537), (u'sprite', u'commercial', -0.5204081632653064), (u')', u'irrelevant', -0.6730769230769231), (u'studios', u'major', 0.5516304347826084), (u'(', u'fictitious', -0.5), (u'god', u'fictitious', -0.5115606936416185), (u'merchandise', u'different', 0.5710248681235869), (u'tie-in', u'different', 0.631195335276968), (u'(', u'different', 0.631195335276968), (u'contradictions', u'logical', -0.5869565217391304), (u'adherence', u'obvious', -0.8155080213903743), (u'formula', u'obvious', -0.6770032086501694), (u\"children's\", u'good', 0.5615063685384902), (u\"children's\", u'most', 0.6009559476084095), (u'entertainment', u'good', 0.5927970485732953), (u'entertainment', u'most', 0.6312800963081864), (u'maker', u'long', -0.520013261799988), (u'percentage', u'good', 0.5541807432432432)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'attempt', u'fully-animated', -0.6500000000000001), (u'attempt', u'feature-length', -0.6500000000000001), (u'cartoon', u\"disney's\", -0.6121365140170645), (u'empire', u\"disney's\", 0.7184592482322291), (u'challenger', u'recent', 0.5461538461538459), (u'challenger', u'other', 0.5243462578899911), (u\"fall's\", u'last', 0.5236598203992476), (u'century', u'20th', 0.6119637251482386), (u'century', u'flawed', 0.7846496322106079), (u'fox', u'20th', 0.5893652561247216), (u'production', u'20th', 0.593389296956978), (u'cast', u'lively', 0.5740075444584153), (u'palate', u'colorful', 0.6702127659574473), (u'piece', u'best', 0.5906301909456378), (u'animation', u'best', 0.7708779279554768), (u'dead', u'much', -0.5258787095474688), (u'arrival', u'much', 0.6291064388961892), (u'kingdom', u'magic', 0.8296428571428571), (u\"that'd\", u'mediocre', -0.6396723217361351), (u'daughter', u'early-teen', 0.5169902912621359), (u'knight', u'belated', -0.7621951219512197), (u'king', u'belated', -0.7941988950276245), (u'warlord', u'evil', 0.6195756991321119), (u'ruber', u'evil', -0.7543021032504783), (u'(', u'evil', -0.5057692307692307), (u'table', u'ex-round', 0.5520833333333333), (u'member-gone-bad', u'ex-round', -0.5), (u'sword', u'magical', 0.6717171717171719), (u'excalibur', u'magical', -0.8272727272727274), (u'forest', u'booby-trapped', -0.5), (u'forest', u'dangerous', -0.5), (u'garrett', u'timberland-dweller', -0.8750000000000001), (u'garrett', u'blind', -0.8519736842105268), (u'(', u'timberland-dweller', -0.5), (u'(', u'blind', 0.5487804878048781), (u'carey', u'timberland-dweller', -0.75), (u'dragon', u'two-headed', 0.6612903225806454), (u'mold', u'sexist', -0.9147727272727273), (u'mold', u'medieval', -0.5526315789473679), (u'showmanship', u'pure', 0.6686746987951809), (u'element', u'essential', 0.7423551502145921), (u\"it's\", u'essential', 0.7258064516129029), (u'ranks', u'high', 0.6917701863354037), (u'disney', u'high', 0.5625696387451408), (u'animation', u'subpar', 0.6011673151750977), (u'songs', u'forgettable', -0.5092378752886839), (u'footage', u'computerized', -0.6454545454545455), (u'footage', u'poorly-integrated', 0.5617283950617282), (u'ogre', u'angry', 0.598441926345609), (u\"herc's\", u'angry', 0.5078125), (u'film', u'much', 0.5160740880865733), (u'signs', u'least', -0.7410259251052516), (u'fans', u'least', -0.6055473680657899), (u'bronson', u'white', -0.8200293829578844), (u'footage', u'same', 0.575680718325462), (u'scenes', u'few', 0.519258630394862), (u')', u'specific', 0.515625), (u'moments', u'musical', 0.6622869318181817), (u'moments', u'big', 0.5070302523326646), (u'jane', u'musical', 0.7608695652173911), (u'mess', u'much', -0.7896147634970856), (u'error', u'grievous', -0.9051724137931033), (u'lack', u'complete', -0.5853803486529319), (u'personality', u'complete', -0.505327545382794), (u'way', u'long', 0.5198665544602067)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'viewing', u'cathartic', 0.7419106317411406), (u'experience', u'cathartic', 0.7293401665598978), (u'godard', u'jean-luc', 0.7000000000000001), (u'godard', u'new', 0.7281344792719923), (u'film', u'jean-luc', 0.5116875712656785), (u'film', u'new', 0.5460290976992845), (u'member', u'founding', -0.8309248554913298), (u'wave', u'new', 0.5612148552245071), (u'wave', u'french', 0.5096032745591939), (u'wave', u'influential', 0.7434017595307919), (u'film', u'esteemed', -0.7411306605134119), (u'critic', u'esteemed', -0.7370689655172414), (u'perspective', u'historical', 0.8357732732732731), (u'writing', u'much', -0.5184747421516224), (u'creation', u'latest', 0.7081605729362987), (u'film', u'exasperating', -0.8267374517374518), (u'experience', u'exasperating', -0.7695810564663026), (u'concept', u'abstract', 0.8004484304932741), (u'categories', u'following', 0.7349780166096731), (u'passion', u'physical', 0.6919368075502665), (u'quarrels', u'physical', -0.7085889570552144), (u'truths', u'universal', 0.8641304347826088), (u'couples', u'different', 0.575797872340426), (u'task', u'self-appointed', 0.531578947368421), (u'breakup', u'recent', 0.5461538461538459), (u'idea', u'central', -0.5270801954938062), (u'value', u'sympathetic', -0.6302852518713331), (u'anyone', u'sympathetic', 0.5157766990291263), (u'vignettes', u'repetitive', 0.5739549839228297), (u'scene', u'next', 0.5080356403675803), (u'something', u'utter', -0.5049051780717817), (u'sections', u'whole', -0.5330788804071245), (u'critics', u'fellow', 0.6413427561837456), (u'conversations', u'few', 0.553735255570118), (u'conversations', u'first', 0.5676114321906423), (u'failures', u'repeated', -0.6371359223300972), (u'footage', u'white', 0.6144920346177167), (u'footage', u'black', 0.5798210161662815), (u'half', u'first', -0.5390773778157941), (u'read', u'edgar', 0.694130127298444), (u'opportunities', u'numerous', -0.5393318965517242), (u'detail', u'crisp', 0.8343023255813952), (u'eye', u'photographic', 0.6436343852013057), (u'times', u'numerous', 0.5621500164698132), (u'times', u'black', 0.5697261830578055), (u'half', u'second', -0.5545316996700956), (u'hyper-color', u'nauseating', -0.6538461538461537), (u'character', u'doomed', 0.6111858070440089), (u'scene', u'only', -0.5171236355509893), (u'states', u'united', 0.7276119402985075), (u'history', u'united', 0.7959893048128343), (u'garbage', u'such', -0.8205944798301485), (u'focus', u'pointless', -0.7381756756756754), (u'qualities', u'specific', 0.5269694819020578), (u'couplehood', u'specific', -0.7380952380952385), (u'person', u'intelligent', 0.6931999822395877), (u'storyteller', u'admirable', 0.8734939759036141)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'mess', u'terrible', -0.926821568379846), (u'mess', u'terrible', -0.926821568379846), (u'grant', u'hugh', -0.5966981132075472), (u'dork', u'huge', -0.7743506493506492), (u'oral-sex/prostitution', u'whole', -0.5330788804071246), (u'thing', u'whole', -0.5700839563066104), (u'(', u'whole', -0.5330788804071246), (u'jim', u\"we're\", 0.5256410256410255), (u'carrey-annoying', u\"we're\", -0.5), (u'smiles', u'nervous', 0.7863175675675673), (u'hand', u'other', 0.5504469064519079), (u'entendres', u'double', -0.837979094076655), (u'entendres', u'obscene', -0.8478260869565217), (u'girlfriend', u'pregnant', -0.5003358409457277), (u'pussy', u'big', -0.6559684684684686), (u'movie', u'cookie-cutter', -0.5244372990353697), (u'movie', u'predictable', -0.6809926546248952), (u'child', u'successful', 0.6499913274745607), (u'psychiatrist', u'successful', -0.6560186454620234), (u'exchange', u'unfunny', -0.8258426966292136), (u'exchange', u'following', 0.5005261081152175), (u'smile', u'nervous', 0.6805555555555552), (u'accent', u'english', -0.5418145750678642), (u'accent', u'annoying', -0.7289231620444017), (u'i-think-i-actually-have-', u'english', 0.574561403508772), (u'jokes', u'stupid', -0.8776238051779626), (u'jokes', u'many', -0.5722444255170456), (u'reaction', u'usual', 0.5366666666666667), (u'(', u'usual', 0.5366666666666666), (u'birth', u'pregnancy/child', 0.5697674418604651), (u'birth', u'possible', 0.5675375502432837), (u'gag', u'pregnancy/child', -0.6836734693877552), (u'gag', u'possible', -0.6856362289312838), (u'friend', u'annoying', -0.6018862911795964), (u'slapstick', u'cacophonous', -0.6980198019801989), (u'none', u'cacophonous', -0.8558648111332011), (u'scene', u'such', 0.5251910665558436), (u'arnie', u'costumed', -0.7388059701492535), (u'parallels', u'own', 0.7876016260162603), (u'character', u'interesting', -0.5146744825131968), (u'character', u'only', -0.5197697790745235), (u'hideaway', u'dreadful', -0.9626865671641791), (u'longing', u'simultaneous', 0.7065217391304341), (u')', u'simultaneous', 0.6249999999999998), (u'doctor', u'russian', -0.6361362553596951), (u'medicine', u'veterinary', -0.8906249999999999), (u'obstetrics', u'veterinary', -0.75), (u'humor', u'much', -0.5196734770259788), (u'character--', u'one-joke', 0.5081106870229007), (u'stereotype', u'foreign-guy-who-mispronounces-english', -0.6041666666666667), (u'stereotype', u'old', -0.5897997496871092), (u'(', u'foreign-guy-who-mispronounces-english', -0.5), (u'(', u'old', 0.5149253731343284), (u'vodka', u'favorite', -0.593922651933702), (u'joke', u'unamusing', -0.9299516908212561), (u'joke', u'nasty', -0.6217391304347826), (u'failure', u'complete', -0.7432516530532868), (u'laughs', u'low', -0.538132911392405), (u'slapstick', u'unfunny', -0.7622579834045765), (u'slapstick', u'loud', -0.5881346777097586), (u'lunacy', u'uninspired', -0.8805774278215222), (u'lunacy', u'other', -0.5877139673857994), (u'jokes', u'caught-with-his-pants-down', -0.6456043956043956), (u')', u'caught-with-his-pants-down', -0.5), (u'people', u'more', 0.5450739773301427), (u'smiles', u'nervous', 0.7863175675675673), (u'anyone', u'unauthentic', -0.7813315926892952), (u')', u'hugh', -0.5476190476190479), (u')', u'sorry', -0.6492537313432836), (u'desire', u'unfulfilled', 0.5305676855895196)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'foray', u'first', 0.6160422753050347), (u'films', u'first', 0.5656778934224339), (u'talent', u'only', -0.6378223528601281), (u'member', u'notable', 0.5563430827202582), (u'member', u'only', -0.5235635702858046), (u'opening', u'very', 0.5756090284560652), (u'male', u'lusty', 0.61006711409396), (u'footage', u'ample', 0.5118749999999997), (u'breasts', u'enormous', -0.829317269076305), (u'cleavage', u'enormous', -0.7453703703703702), (u'extras', u'female', -0.6172209417188017), (u'war', u'civil', 0.7488458806818178), (u'war', u'american', 0.771693313416059), (u'war', u'second', 0.671571644845198), (u'resistance', u'former', 0.6735159817351599), (u'fighter', u'former', -0.5651570177312537), (u'uniforms', u'nazi-style', -0.5909090909090907), (u'males', u'helpless', -0.5442073170731708), (u'lover', u'former', 0.5591459237268782), (u'side', u'other', 0.6151884543114747), (u'areas', u'government-controlled', -0.6346153846153846), (u'jet', u'private', 0.533670621074669), (u'jet', u'modern', 0.5656740228662592), (u'moments', u'significant', 0.5093772760378733), (u'teenagers', u'young', 0.5711792086889061), (u'horse', u'dark', 0.5325400679941718), (u'comics', u'dark', -0.5603448275862066), (u'version', u'dark', 0.6408582089552238), (u'stunts', u'own', 0.5259295499021528), (u'leather', u'strapless', -0.9051724137931033), (u'leather', u'skimpy', -0.9527220630372494), (u'top', u'strapless', -0.7295345104333872)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'couples', u'teen', -0.5900252070579758), (u'movie', u'mind-fuck', -0.5244372990353697), (u'idea', u'cool', -0.6241430313205814), (u'package', u'bad', -0.6816575886942443), (u'(', u'such', 0.5144472361809046), (u'highway', u'such', -0.6984939759036142), (u'ways', u'bad', -0.5983229873442086), (u'ways', u'good', 0.5751422562160596), (u'concept', u'neat', -0.5504281636536628), (u'problem', u'main', -0.5852155208074792), (u'apparitions', u'strange', 0.5028409090909092), (u'scenes', u'chase', -0.5506875837760019), (u'things', u'weird', -0.5238913076771612), (u'clue', u'same', -0.6496651518775411), (u'problem', u'biggest', -0.6092005900354458), (u'secret', u'big', 0.5041692000347434), (u'minutes', u'final', 0.5070913100130299), (u'part', u'sad', 0.5051845509893456), (u'point', u'half-way', -0.5525793650793651), (u'bit', u'little', 0.5325569647518579), (u'sense', u'little', 0.5410261918401119), (u'line', u'bottom', -0.6195762947407433), (u'movies', u'bottom', -0.6004718565800327), (u'audience', u'sure', 0.5001409816836596), (u'password', u'secret', -0.6673151750972761), (u'scenes', u'different', 0.6377705327933281), (u'insight', u'further', -0.5), (u'movie', u'mind-fuck', -0.5244372990353697), (u'movie', u'teen', -0.5572713198921155), (u'movie', u'decent', -0.6460103701569926), (u'edge', u'little', 0.6717440837140896), (u'sense', u'more', 0.5526909514702365), (u'part', u'most', 0.569710378878404), (u'character', u'same', 0.5223184016634269), (u'character', u'exact', -0.5503178725641722), (u'neighborhood', u'new', -0.5183489396860363), (u'kudos', u'biggest', 0.81198347107438), (u'film', u'entire', 0.5615437119617769), (u'runtime', u'most', -0.7836828016952934), (u'ending', u'cool', 0.5504507030104374), (u'explanation', u'cool', -0.6887673850727252), (u'slasher', u'teen', -0.6654759606101561), (u'flick', u'teen', -0.5573727933541022), (u'kids', u'hot', 0.5246128213866486)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'creatures', u'beautiful', 0.6863616745791974), (u\"god's\", u'beautiful', 0.6118421052631579), (u'earth', u'green', 0.5294864284319468), (u'(', u'green', 0.5240963855421686), (u'guy', u'large', 0.5188503440366974), (u')', u'large', 0.5551948051948055), (u'piece', u'predictable', -0.6424599905860201), (u'piece', u'slow-moving', 0.518604651162791), (u'fluff', u'predictable', -0.677846312002056), (u'adaptation', u'novel', 0.6042510121457485), (u'adaptation', u'horrific', 0.7073353293413169), (u'alicia', u'novel', -0.7935684647302905), (u'alicia', u'horrific', -0.7083333333333337), (u'role', u'minor', 0.6355615285730981), (u'waste', u'unfunny', -0.9545454545454546), (u'waste', u'annoying', -0.8947048974466305), (u'time', u'unfunny', -0.8002240215382143), (u'movie', u'good', -0.5202191096935236), (u'one', u'only', -0.5216873493292249), (u'mind', u'open', 0.584841325242979), (u'course', u'good', -0.5158995901325563), (u'jokes', u'bad', -0.7831658926177106), (u'characters', u'unlikable', -0.7910543717854517), (u'characters', u'whiny', -0.5620854063018247), (u'way', u'same', 0.534826079063857), (u\"f$&#in'\", u'worst', -0.8138075313807531), (u'movie', u'worst', -0.828178151392134), (u\"i've\", u'worst', -0.8138075313807531), (u'night', u'last', 0.5171018296009081), (u'thriller', u'made-for-video', -0.5717821782178218), (u'compulsion', u'inner', -0.5771276595744683), (u\"i'll\", u'inner', 0.6309523809523809), (u'alicia', u'worst', -0.9562438544739429), (u'alicia', u'regret--the', -0.7556293470422666), (u'silverstone', u'worst', -0.9710223572823964), (u'silverstone', u'regret--the', -0.8258230583449852), (u'movie', u'worst', -0.828178151392134), (u'paragraph', u'last', -0.5825256161728057), (u'shred', u'thin', -0.6684210526315788), (u'glacier', u'slower', 0.5769230769230779), (u'alicia', u'old', -0.8248730964467005), (u'minutes', u'last', -0.5716504527066075), (u'sequence', u'violent', 0.5794154036498608), (u'star', u'four-', 0.5640065583830325), (u'star', u'other', 0.5286725386878867), (u'features', u'four-', 0.5709692874073328), (u'features', u'other', 0.5357350572080027), (u'cocktail', u'drunk', -0.8002577319587632), (u'alicia', u'beautiful', -0.7517793594306048), (u'movie', u'least', -0.6230484515346721), (u'critic', u'least', -0.5834270655911562), (u'guy', u'large', 0.5188503440366974), (u'star', u'mostly-silent', 0.5043383947939263), (u\"character's\", u'male', 0.5569333844194307), (u'fantasies', u'male', 0.6018895348837215), (u'father', u'drunken', 0.5770070457477428), (u\"she's\", u'drunken', -0.5223872826351048), (u'youth', u'lost', 0.6671105765100094), (u'boy', u'prepubescent', -0.7255965292841653), (u\"she's\", u'drunken', -0.5223872826351048), (u\"wife's\", u'aged', 0.6503281378178837), (u\"wife's\", u'middle', 0.5377185927040631), (u'fantasies', u'aged', 0.6349557522123892), (u'fantasies', u'middle', 0.5210416666666673), (u'counterpart', u'male', -0.577994428969359), (u'pound', u'200+', -0.5892857142857139), (u'woman', u'200+', -0.6019977490151943), (u'silk', u'black', -0.7358974358974362), (u'teddy', u'black', 0.518456375838926), (u'none', u'least', -0.7478972689041252), (u'staple', u'late-night', 0.5277777777777775), (u'staple', u'cinemax', -0.9306569343065694)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'days', u'early', -0.5016903047265637), (u'publicity', u'pre-release', 0.6136363636363634), (u'publicity', u'aborted', -0.7589285714285714), (u'rain', u'hard', -0.6093140442132645), (u'moniker', u'appropriate', -0.6653543307086616), (u'pictures', u'paramount', 0.6427251732101611), (u'movie', u'nervous', 0.6589231160896128), (u'disaster', u'other', -0.5547245903313083), (u'name', u'other', -0.5271490487155125), (u'passing', u'worse', -0.7196061643835615), (u'shot', u'best', 0.5910240817319385), (u'dam', u'overworked', -0.8448275862068966), (u'skies', u'cloud-choked', 0.6111111111111108), (u'entertainment', u'initial', 0.6804706601466994), (u'value', u'initial', -0.5924487259167188), (u'motion', u'95-minute', 0.5440251572327044), (u'motion', u'longest-seeming', 0.5440251572327044), (u'pictures', u'95-minute', 0.5707547169811319), (u'pictures', u'longest-seeming', 0.5707547169811319), (u'rain', u'hard', -0.6093140442132645), (u'chase', u'dull', -0.8322991002171892), (u'chase', u'extended', -0.6414068165337204), (u'sequence', u'dull', -0.7657543503970263), (u'shoot-outs', u'occasional', -0.5491803278688525), (u'glass', u'broken', 0.544022770398482), (u'gunfire', u'broken', -0.6781946072684644), (u'characters', u'real', 0.5230799464668627), (u\"it's\", u'preposterous', -0.7749999999999999), (u'enjoyment', u'masochistic', -0.6397058823529409), (u'enjoyment', u'sheer', 0.7059765208110989), (u'way', u'idiotic', -0.8726755370310997), (u'subplots', u'myriad', -0.644230769230769), (u'rain', u'hard', -0.6093140442132645), (u'mikael', u'cinematographer-turned-director', -0.9000000000000001), (u'rain', u'hard', -0.6093140442132645), (u'repetition', u'tedious', -0.7613636363636366), (u'kinds', u'same', 0.5653095843935535), (u'things', u'same', 0.5293188383882598), (u'material', u'worth', -0.5519132057862806), (u'length', u'acceptable', -0.6180981595092022), (u'characters', u'paper-thin', 0.5046583850931676), (u'circumstances', u'contrived', -0.5184824902723737), (u'action', u'dimensional', -0.5732583910708227), (u'action', u'one-', -0.544476348278422), (u'hero', u'dimensional', -0.5229885057471264), (u'hero', u'one-', 0.5061902082160944), (u'car', u'armored', -0.7879662522202486), (u'cash', u'full', -0.5885285260844795), (u'love', u'would-be', 0.5441400304414004), (u'interest', u'would-be', -0.5426540284360191), (u'couple', u'old', 0.5243753235141876), (u'relief', u'comic', 0.5627507755946222), (u'sheriff', u'easily-corrupted', -0.6142857142857143), (u'randy', u'easily-corrupted', -0.7083333333333334), (u'helpfulness', u'seeming', -0.6617647058823539), (u'motives', u'ulterior', 0.6142857142857139), (u'motives', u'sinister', 0.7496290801186943), (u'acting', u'real', -0.5152532040887416), (u'one-liners', u'lame', -0.845), (u'one-liners', u'few', 0.5121776504297995), (u'characters', u'lifeless', -0.8186788663244814), (u'actor', u'lifeless', -0.8121638035745272), (u'performances', u'legitimate', 0.7543914680050183), (u'thespians', u'cinematic', 0.5081300813008129), (u'thespians', u'best', 0.5725146198830406), (u'today', u'cinematic', 0.6853499592706312), (u'paycheck', u'good', -0.7636286442117182), (u'time-to-time', u'good', 0.5042265426880812), (u'disaster', u'natural', 0.6516116195781938), (u'movie', u'natural', 0.6996316758747698), (u'engine', u'chief', -0.637768817204301), (u'conflict', u'chief', -0.5717366628830876), (u'guys/bad', u'good', 0.5042265426880812), (u'guys/bad', u'idiotic', -0.881578947368421), (u'guys', u'good', -0.5474580849145413), (u'guys', u'idiotic', -0.9015689418231793), (u'story', u'good', 0.5301632632925011), (u'brand', u'unique', 0.6844660194174758), (u'mayhem', u'moronic', -0.5815660685154977), (u'fire', u'moronic', -0.5851936966513458), (u'rain', u'hard', -0.6093140442132645)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'movie', u'jaded', -0.6006039181028135), (u'viewer', u'jaded', 0.5125000000000003), (u'invention', u'thankful', 0.6499999999999997), (u\"1960's\", u'late', 0.6567609368095448), (u'television', u'late', -0.5382882882882882), (u'show', u'late', -0.5457145603999423), (u'name', u'same', -0.5372503123339605), (u'evidence', u'wrong', -0.61651560407869), (u'music', u'cool', 0.5119611825772961), (u'hair', u'nice', 0.5562920089619118), (u'outfits', u'cute', -0.6536354581673307), (u'car', u'cute', -0.5837847745509349), (u'movie', u'cool', -0.5715735526475711), (u'minutes', u'first', -0.5465775758758116), (u'production', u'slick', 0.688711911357341), (u'hair', u'nice', 0.5562920089619118), (u'hair', u'complete', -0.54026010743568), (u'costumes', u'nice', 0.6301623469097126), (u'cop', u'hour-long', -0.5866141732283463), (u'show', u'hour-long', -0.5074786324786323), (u'clich', u'single', -0.7444292080905041), (u'plot', u'most', -0.535359001817331), (u'man', u'crazy', -0.5109185051778475), (u'man', u'resident', 0.531823745410037), (u'thing', u'only', -0.5649436888392133), (u'watching', u'worth', -0.5099013859275052), (u'watching', u'only', -0.5827730124605126), (u'mess', u'convoluted', -0.8918035847647497), (u'cast', u'young', 0.6021710669866898), (u'clothes', u'cool', -0.5676145136387032), (u'hair', u'nice', 0.5562920089619118), (u'mindset', u'teenage', -0.7157953281423806), (u'mindset', u'older', -0.5970930232558136), (u'audience', u'spoon-fed', -0.5070532915360502), (u'wisdom', u'questionable', 0.5224719101123592), (u'films', u'memorable', 0.8123042745108534), (u'indication', u'clear', 0.6271647699158832), (u'film', u'clear', 0.5986341853854759), (u'attempt', u'more', -0.6397164577768646), (u'spending', u'teenage', -0.5334586466165415), (u'dollar', u'teenage', -0.6534275127373785), (u'teen-flicks', u'awful', -0.8414634146341464)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'books', u'successful', 0.6964094796863868), (u'movies', u'successful', 0.5716714359205195), (u'successes', u'early', -0.5916319082377477), (u'films', u'recent', 0.5852401168881293), (u'park', u'jurassic', 0.6220379146919429), (u'park', u'such', 0.5611565717865392), (u'films', u'recent', 0.5852401168881293), (u'turn', u'wrong', 0.5024972889556224), (u'director', u'good', -0.520253471761688), (u'barry', u'good', -0.5421027071061535), (u'disappointment', u'huge', -0.6675701530612246), (u'budget', u'huge', -0.614529653244825), (u'sphere', u'massive', -0.9446564885496185), (u'inside', u'massive', -0.5204549799273561), (u'intelligence', u'alien', 0.5069511025886863), (u'jerry', u'alien', -0.5067500985415845), (u'crap', u'weird', -0.7192495372707387), (u'way', u'wrong', -0.5320378547705313), (u'movie', u'long', -0.5252190661643312), (u')', u'embarrassed', -0.6730769230769231), (u'lines', u'dull', -0.8395169396322354), (u'person', u'only', 0.5324251591318896), (u'mathematician', u'funny', -0.7099782029547104), (u'hoffman', u'dull', -0.7484990864004175), (u'peter', u'same', 0.5195031460730491), (u'crichton', u'better', -0.7461033634126333), (u'adaptation', u'better', -0.5075721862109606), (u'disclosure', u'better', -0.8581576535288729), (u'speed', u'godawful', -0.8645320197044335), (u'plots', u'great', -0.5168666418487442), (u'dialogue', u'terrible', -0.8232632643884891), (u'effect', u'special', 0.5556096610829763), (u'effect', u'nice', 0.5785009624639076), (u'effect', u'few', 0.5447860962566845), (u'shots', u'special', 0.5047009117931933), (u'shots', u'nice', 0.5279867761057911), (u'guess', u\"anyone's\", -0.6954545454545454), (u'squid', u'giant', -0.896516393442623), (u'attack', u'giant', -0.6169472152950956), (u'squid', u'giant', -0.896516393442623), (u'budget', u'massive', -0.6838083791208791), (u'scene', u'whole', -0.522351206987756), (u'squid', u'stupid', -0.9649822695035462), (u'plot', u'own', -0.5115764828303849), (u'sci-fi', u'hokey', -0.6714285714285715), (u'horizon', u'shining/event', -0.5624999999999998), (u'thriller', u'psychological', 0.5009652509652509), (u'thriller', u'shining/event', -0.5717821782178218), (u'jump', u'few', -0.5649889380530975), (u'scenes', u'few', 0.519258630394862), (u'bit', u'interesting', 0.5103538167732812), (u'waste', u'big', -0.8523623817123334), (u'talent', u'fine', -0.5400634018364671), (u'movie', u'good', -0.5202191096935236)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'feature', u'musical', 0.6612903225806454), (u'feature', u'animated', 0.6445464663327766), (u'story', u'classic', 0.635963101459191), (u'results', u'obvious', -0.6014243956757761), (u'schoolteacher', u'british', 0.6322314049586774), (u'ruler', u'greedy', 0.6957295373665482), (u'minister', u'prime', 0.7551194539249146), (u'minister', u\"king's\", 0.7351973684210521), (u'villain', u'stereotypical', -0.6685682146874498), (u'plots', u'stereotypical', -0.7116368286445016), (u'subplot', u'main', -0.6515166151468316), (u'subplot', u'predictable', -0.7855499727586416), (u'deals', u'main', 0.5815151776474646), (u'laws', u'ancient', 0.6216350947158531), (u'saim', u'ancient', -0.6971830985915495), (u'character', u'strong', 0.6672425577156743), (u'character', u'lone', 0.6170702718763925), (u'anna', u'strong', 0.8731343283582094), (u'waste', u'horrible', -0.902074898785425), (u'talent', u'horrible', -0.741955346147668), (u'pages', u'blank', -0.5512820512820511), (u'difficulties', u'major', 0.6772455089820352), (u'dialogue', u'worst', -0.8599484653710112), (u'memory', u'recent', 0.6612656289869866), (u'king', u'obnoxious', -0.5250689179875947), (u'film', u'short', 0.5467957812542819), (u'display', u'classic', 0.5541871921182266), (u'filmmaking', u'terrible', -0.676635822868473), (u'dialogue', u'repetitive', -0.8141186575052851), (u'song', u'sole', -0.5258604989983607), (u'outdoors', u'great', 0.7596073166638694), (u'features', u'animated', 0.6183735860593094), (u'features', u'disney', 0.5379781420765025), (u'horror', u'sheer', 0.5951853901494186), (u'song', u'happy', 0.692035535354579), (u'dialogue', u'terrible', -0.8232632643884891), (u'voice', u'awful', -0.792473273620341), (u'track', u'awful', -0.8539118065433855), (u'signs', u'mild', -0.6815789473684216), (u'thing', u'only', -0.5649436888392133), (u'changes', u'only', 0.5317535797434457), (u'film', u'more', 0.5229000443069107), (u'plot', u'whole', -0.6114140778984944), (u\"minister's\", u'prime', 0.7551194539249146), (u'sidekick', u'hideous', -0.5434782608695652), (u'sidekick', u'prime', 0.5899999999999995), (u'films-', u'animated', 0.6447654669669098), (u\"1994's\", u'such', 0.5607954545454547), (u'movies', u'favorite', 0.53328509406657), (u'settings', u'animated', 0.7253209384683484), (u'king', u'annoying', -0.5651317102374775), (u'seconds', u'fourth', -0.698396793587174), (u'movie', u'worst', -0.828178151392134), (u'line-', u'bottom', -0.6195762947407433), (u'children', u'young', 0.6335183513039783), (u'bit', u'slightest', -0.6314835867557198)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'film', u'last', 0.5125176500001422), (u'drama', u'serious', 0.5478686951754386), (u'picture', u'latest', 0.5778943991199927), (u'type', u'worst', -0.8103975895106048), (u'romance', u'worst', -0.7762077804094611), (u'type', u'same', 0.519798311320505), (u'story', u'same', 0.5401087695445274), (u'times', u'many', 0.6261262492245719), (u'pace', u'deliberate', 0.8904382470119524), (u'statements', u'meaningful', -0.5892857142857139), (u'mother', u'single', 0.6060741666385219), (u'researcher', u'single', 0.5810600155884644), (u'love', u'anonymous', 0.6584948688711519), (u'letter', u'anonymous', 0.6017786561264822), (u'letter', u'anonymous', 0.6017786561264822), (u'research', u'heavy', -0.5379769299023958), (u'banks', u'outer', 0.5702247191011234), (u'carolina', u'north', 0.6547619047619043), (u'man', u'rugged', 0.6409413973447767), (u'man', u'handsome', 0.6323731326889276), (u'wife', u'late', 0.532592737032874), (u'years', u'few', 0.5750616382065565), (u'melodrama', u'artificial', -0.6212121212121213), (u'melodrama', u'deep', -0.5640553803014368), (u'things', u'positive', -0.5325868849827436), (u'father', u'loving', 0.7319593109803647), (u'dog', u'shaggy', -0.7076271186440679), (u'love', u'shaggy', -0.5826848249027237), (u'advantage', u'great', 0.6272166251533511), (u'coast', u'eastern', 0.7100515463917532), (u'coast', u'beautiful', 0.7614342179559572), (u'city', u'alluring', 0.610036832412523), (u'films', u'less-than-stellar', 0.5397111913357401), (u'luck', u'bad', -0.7422990444069704), (u'luck', u'such', -0.5782780195865074), (u'centerpieces', u'ridiculous', -0.9380403458213257), (u'sea', u'storm-swept', 0.5288461538461537), (u'actors', u'other', 0.5233869093658627), (u'actress', u'underused', -0.5131233595800526), (u'half', u'first', -0.5390773778157941), (u'television', u'trite', -0.6630434782608694), (u'television', u'hum-drum', -0.5), (u'movie', u'trite', -0.6845397000137607), (u'deal', u'great', 0.6738867021120346), (u'time', u'great', 0.5987358553474668), (u'bit', u'least', -0.5676081120711046), (u'stimulating', u'least', 0.6183669976035601), (u'sequences', u'dialogue-laden', -0.5579999999999999), (u'falseness', u'resounding', -0.9166666666666665), (u'characters', u'main', 0.5076791008368673), (u'interest', u'little', -0.5431923314144737), (u'novel', u'familiar', 0.6065833390096041), (u'hollywood', u'tried-and-true', -0.5025575447570334), (u'melodrama', u'tried-and-true', -0.6212121212121213), (u'secret', u\"theresa's\", -0.597560975609756), (u'obstacles', u'several', 0.6391684901531732), (u'exact', u'same', -0.5442621651608813), (u'problem', u'same', -0.5743003576030461), (u\"year's\", u'last', 0.5301914041543155), (u'(', u'similar', 0.6279761904761905), (u'cage', u'ryan-nicolas', -0.6568627450980391), (u'drama', u'romantic', 0.5746774685087364), (u'drama', u'ryan-nicolas', 0.5717299578059072), (u'letter', u'heartfelt', 0.690476190476191)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'movie', u'wierd', 0.6018066406249999), (u'hour', u'first', -0.5495189196643576), (u'stock', u'standard', -0.6770161290322577), (u'aliens', u'standard', -0.5389738529847067), (u'clone', u'standard', -0.6917608457892815), (u'atmosphere', u'eerie', 0.7453266120562915), (u'hour', u'half', -0.6377178862162536), (u'hour', u'last', -0.574555750352773), (u'way', u'wrong', -0.5320378547705313), (u'tricks', u'clever', 0.6290392383150608), (u'noises', u'loud', -0.9130602782071098), (u'camera', u'sudden', -0.5558123000355495), (u'shifts', u'sudden', 0.5229624838292365), (u'bursts', u'quick', -0.8262492820218266), (u'bursts', u'short', -0.7848421679571171), (u'crew', u'previous', 0.5709139708998445), (u'crew', u'new', 0.538072785927672), (u'dialogue', u'interesting', -0.6061313936440416), (u'dialogue', u'complex', 0.6278286992369095), (u'camerawork', u'nice', 0.5979729729729729), (u'sequences', u'certain', -0.5134466874407264), (u'objects', u'specific', 0.5842274678111582), (u'cast', u'experienced', 0.5368233650335783), (u'cast', u'good', 0.5285829982978775)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'marks', u'placid', -0.6080508474576273), (u'claims', u'placid', 0.5259365994236311), (u'guy', u'same', -0.5223246784760981), (u'seriousness', u'deadpan', 0.7453703703703709), (u'seriousness', u'funny', 0.5571618477899636), (u'game', u'croc-hunting', 0.5530303030303029), (u'warden', u'croc-hunting', -0.7), (u'guest', u'unwelcome', -0.6111111111111107), (u'hector', u'unwelcome', -0.9339622641509434), (u'band', u'merry', -0.584851936218679), (u'mrs', u'merry', -0.5992187499999995), (u'(', u'bickerman', -0.7500000000000003), (u'betty', u'bickerman', -0.8333333333333335), (u')', u'white', 0.5542986425339365), (u'lady', u'old', -0.5429147465437788), (u'lady', u'weird', -0.5957920792079208), (u'music', u'-like', 0.5431069479318589), (u'plays', u'-like', -0.5197578912181845), (u'cousin', u'slippery', -0.5799999999999998), (u'anaconda', u'slippery', -0.7), (u'plot', u'formulaic', -0.6667286990380212), (u\"it's\", u'funny', -0.5266798418972333), (u'horror', u'several', 0.5753032600454892), (u'films', u'several', 0.6055527205460228), (u'characters', u'major', 0.5351262349066958), (u'course', u'computer-generated', -0.5201207243460766), (u'monster', u'thirty-foot', -0.5357142857142858), (u'crocodile', u'thirty-foot', -0.5833333333333335), (u'explanation', u'little', -0.647047277213841), (u'crocodile', u'giant', -0.6340579710144928), (u'mumbo', u'semi-mystical', -0.8333333333333331), (u'crocodiles', u'much', -0.8308801884015309), (u'myth', u'urban', 0.5666666666666669), (u'alligators', u'urban', -0.5), (u'explanation', u'much', -0.6425286832999455), (u'mutant', u'radioactive', -0.9210526315789475), (u'space', u'outer', 0.5114929449248976), (u'hand', u'other', 0.5504469064519079), (u'efforts', u'best', 0.5884873003454), (u'script', u'sorry', -0.7494274226384365)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'movie', u'actual', -0.5046049603681924), (u'inside', u'actual', 0.6075219815569378), (u'sets', u'lavish', 0.607981220657277), (u'costumes', u'flashy', 0.6094104308390021), (u'fight', u'confusing', -0.5658705253370524), (u'fight', u'big', -0.5125953324652652), (u'scenes', u'confusing', -0.5795269688367197), (u'kings', u'cheesy', 0.5065406976744186), (u'island', u'cheesy', -0.7445367527541991), (u'stage', u'cheesy', -0.5764307917352348), (u'fun', u'much', 0.5393307031053461), (u'mood', u'striking', 0.8018867924528299), (u'piece', u'striking', 0.6422811059907835), (u'feel', u'otherworldly', -0.6206955335833618), (u'creature', u'brooding', -0.6254882812500004), (u'creature', u'creepy', 0.523743315508021), (u'repellent', u'charismatic', 0.7656249999999996), (u'sides', u'opposite', 0.6403559964983953), (u'coin', u'same', 0.6628289473684212), (u'demons', u'inner', 0.6309523809523809), (u'skyline', u'gothic', 0.6212121212121217), (u'skyline', u'breathtaking', 0.7992700729927006), (u'knight', u'dark', 0.6944832402234636), (u'ice', u'psychedelic', 0.6294363256784967), (u'capades', u'psychedelic', -0.6250000000000004), (u'tv', u'campy', -0.625), (u'window-dresser', u'former', 0.553125), (u'schumacher', u'former', -0.8567033678756475), (u'sets', u'adept', 0.6415125495376484), (u'costumes', u'exotic', 0.6505898681471202), (u'notion', u'faintest', -0.9279069767441861), (u\"who's\", u'difficult', 0.5655652134443864), (u'one-liners', u'other', 0.5243462578899911), (u'puns', u'bad', -0.8222602739726026), (u'confetti', u'like', -0.516355140186916), (u'jokes', u'few', -0.6343798311333095), (u'wisecracks', u'many', 0.6499714611872145), (u'occasions', u'several', 0.5669642857142857), (u'batman', u'best', -0.6113175989593014), (u'eyes', u'expressive', -0.5069605568445478), (u'smile', u'weary', 0.6599999999999994), (u'scene', u'effective', 0.7349401588390149), (u'exchange', u'quiet', 0.6689189189189186), (u'clooney', u'quiet', 0.682921027592769), (u'father', u'surrogate', 0.7019839946648877), (u'heroes', u'other', 0.5182595396329387), (u'hell', u'horny', -0.7472906403940891), (u'characters', u'major', 0.5351262349066958), (u'batgirl', u'unnecessary', -0.9525939177101967), (u'nipples', u'built-in', -0.75), (u'buttocks', u'shapely', -0.9375), (u'codpieces', u'huge', -0.7743506493506496), (u'stars', u'real', 0.5125352339153627), (u'schwarzenegger', u'arnold', -0.8460183227625089), (u'performance', u'worst', -0.7522594428031386), (u'years', u'worst', -0.7722623659117998), (u'catch', u'lame', -0.9069895199459095), (u'phrases', u'lame', -0.8298671288272677), (u'fashion', u'wooden', -0.5399294065740128), (u'scenes', u'early', -0.518516356029874), (u'poison', u'eco-psychotic', -0.7352941176470589), (u'ivy', u'eco-psychotic', -0.8749999999999999), (u'mae', u'nice', 0.5462962962962962), (u'west', u'nice', -0.5561081356796735), (u'impersonation', u'nice', 0.5954128440366975), (u'vamp', u'classic', -0.8510971786833856), (u'sense', u'maniacal', 0.5415647921760395), (u'style', u'maniacal', -0.5035587188612103), (u'scenes', u'latter', 0.5451341212240095), (u'lesson', u'biggest', 0.6518102900698713), (u'feelings', u'hard', 0.5470706059088634), (u')', u'hard', -0.5122767857142859), (u'time', u'next', 0.5068072856354369)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'school', u'high', 0.5744165399472971), (u'school', u'cinematic', 0.5466893319250068), (u'school', u'typical', 0.6072297044519265), (u'books', u'black', 0.6498000934627968), (u'books', u'little', 0.6323083182180329), (u'acts', u'sexual', -0.5966981132075472), (u'place', u'sexual', 0.5206026764224513), (u'guys', u'other', -0.5274349122874198), (u'sequel', u'bland', -0.8773695754347486), (u'moment', u'original', 0.507438330842097), (u'moment', u'single', -0.5079330485572312), (u'mediocrity', u'stupefying', -0.9817073170731707), (u'performances', u'scary', 0.5913220145970288), (u'performances', u'least', 0.5514834817984149), (u'memory', u'recent', 0.6612656289869866), (u'movie', u'lifeless', -0.8353300080163892), (u'movie', u'lame', -0.8573850345891605), (u'rachel', u'lifeless', -0.8529411764705883), (u'students', u'fellow', 0.6734234234234237), (u'delight', u'fellow', 0.7916926920674571), (u'ending', u'obligatory', 0.5390577813569557), (u'bloodbath', u'obligatory', -0.6394230769230769), (u'finale', u'big', -0.5212773991353108), (u'scenes', u'repugnant', 0.7059200410046129), (u'stomachs', u\"audience's\", 0.6183641368462445), (u'suicide', u'realistic', 0.7508029978586727), (u'suicide', u'horrible', -0.6368670886075949), (u'points', u'major', 0.5373529411764705), (u'book', u'black', 0.5252618419648437), (u'puppy', u'helpless', -0.5399999999999998), (u'body', u'little', 0.5039826266195524), (u'body', u'poor', -0.6766078184110971), (u'kids', u'few', 0.5325997150533788), (u'girlfriend', u'nonplussed', -0.7680965147453086), (u'energy', u'devoid', -0.6309475396502648), (u'button', u'forward', -0.7275711159737418), (u'button', u'fast', -0.7304687499999998), (u\"film's\", u'bad', -0.6542288446420976), (u'epilogue', u'mandatory', 0.6071428571428572), (u'rest', u'predictable', -0.6773534561827732), (u'violence', u'graphic', 0.6435341318757996), (u'alcohol', u'teen', -0.8149038461538461), (u'abuse', u'teen', -0.5697148817802503), (u'teenagers', u'older', 0.5486869031377898)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'loser', u'drunken', -0.64364406779661), (u'presence', u'boozed-out', -0.5179640718562875), (u'presence', u'smelly', -0.8826530612244899), (u'actor', u'swedish', 0.5154761904761908), (u'actor', u'ever-reliable', 0.5154761904761905), (u'depth', u'swedish', 0.5884615384615383), (u'aberdeen', u'forgettable', -0.583333333333333), (u'aberdeen', u'plodding', -0.95), (u'drama', u'european', 0.6795133729569091), (u'drama', u'mundane', 0.7609601449275364), (u'reflection', u'accurate', 0.7798913043478263), (u'father', u'alcoholic', -0.6486723278069824), (u'yuppie', u'hostile', -0.6875000000000001), (u'daughter', u'hostile', 0.5934249628528977), (u'trip', u'long', -0.5106843575418996), (u'norway', u'long', 0.6242649941199531), (u'days', u'few', 0.5360400679581762), (u'road', u'open', -0.539775910364146), (u'kaisa', u'open', -0.7268041237113405), (u'while', u'other', 0.5626925595324522), (u'stops', u'periodic', 0.7299999999999999), (u'tomas', u'periodic', 0.6249999999999998), (u'kid', u'rotten', -0.5288461538461537), (u'viewpoint', u'sloshed', 0.7065217391304341), (u'tomas', u'sloshed', 0.6249999999999998), (u'relationships', u'personal', 0.742609063955951), (u'indifference', u'personal', 0.7749125874125871), (u'temper', u'vindictive', 0.7012195121951221), (u'temper', u'quick', 0.7437199358631745), (u'notes', u'true', 0.7436494140077385), (u'empathy', u'familial', 0.5596026490066224), (u'empathy', u'unspoken', 0.6499999999999997), (u'dialogue', u'bitchy', -0.7850649350649348), (u'screenwriters', u'bitchy', -0.8065693430656935), (u'cops', u'nosy', -0.5869565217391304), (u'tires', u'flat', -0.5410367170626353), (u'figure', u'flat', -0.7309764309764311), (u'narrative', u'convenient', -0.5397727272727277), (u'narrative', u'schematic', -0.6785714285714289), (u'past', u'dark', 0.6483050847457628), (u'devices', u'simplistic', -0.7042360060514372), (u'strindberg', u'bad', -0.856073211314476), (u'strindberg', u'many', -0.6877998629198081), (u'wannabe', u'bad', -0.7950448130215308), (u'sake', u'own', -0.5760534429599176), (u'casting', u'unimaginative', -0.8824481865284972), (u'role', u'pivotal', 0.7930773117736231), (u'kaisa', u'pivotal', 0.5243902439024389), (u'actress', u'stronger', 0.5825537885874652), (u'coast', u'able', 0.7267251635930997), (u'ghost', u'pastoral', -0.5), (u'ghost', u'own', 0.568181818181818), (u'world', u'pastoral', 0.6008264462809919), (u'world', u'own', 0.6644852295992981), (u'flick', u'indie', -0.7303068649161657), (u'flick', u'american', 0.6099367479733904), (u'flick', u'superior', -0.6720072551390569), (u')', u'intentional', -0.6176470588235294), (u')', u'indie', -0.7105263157894737), (u'acting', u'busy', 0.5402831760133259), (u'brow', u'furrowed', -0.8125000000000002), (u'insouciance', u'last', -0.624221022019111), (u'aberdeen', u'worthwhile', -0.5921052631578947), (u'film', u'earlier', 0.5833767385399494), (u'figures', u'parental', -0.6503378378378379), (u'figures', u'disturbed', 0.5769230769230772), (u'wedlock', u'ceremonial', -0.7352941176470589), (u'diva', u'luminous', -0.7), (u'preening', u'luminous', -0.875), (u'hospital', u'static', -0.557549504950495), (u'performance', u'solid', 0.771630106422888), (u'tomas', u'solid', 0.7012578616352204), (u'chance', u'much', 0.5095702920667556), (u'sorrow', u'catatonic', 0.75), (u'ferocity', u'genuine', -0.5018621973929234), (u'zone', u'gray', -0.814115308151093), (u'complications', u'gray', -0.6562500000000002), (u'romance', u'torn', 0.6250000000000006), (u'years', u'torn', 0.630275229357798), (u'curiosity', u'stifled', -0.8448275862068965), (u'territory', u'neurotic', 0.6769578313253019), (u'addition', u'neurotic', 0.717132386623912), (u\"it's\", u'least', -0.5998116760828625), (u'drama', u'traditional', 0.7661780104712038), (u'movies', u'other', 0.5250239501738377), (u'stories', u'driven', 0.7064422456442249), (u'life', u'real', 0.6131586239277098), (u'answer', u'depressing', -0.6571934207786798), (u'canyon', u'grand', -0.7479999999999998), (u'canyon', u'useful', -0.9145569620253166), (u'films', u'foreign', 0.5397111913357401)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u\"bastard's\", u'happy', 0.7148266241530492), (u'movie', u'quick', -0.5552775058025974), (u'movie', u'happy', 0.5461499690233426), (u'review', u'quick', -0.5692937339271154), (u'tech', u'russian', 0.5432513049962713), (u'ship', u'russian', 0.5093930635838152), (u'action', u'few', -0.5385281526337952), (u'sequences', u'few', -0.545952174337814), (u'thing', u'flashy', -0.5130086031337022), (u'thing', u'pink', 0.6247035573122536), (u'thing', u'big', -0.5705850189951283), (u'sutherland', u'donald', -0.7589020771513352), (u'star', u'real', 0.5227606027328707), (u'good', u'occasional', -0.5449911931542376), (u'shot', u'occasional', -0.5302821748107367), (u'ship', u'sunken', 0.5198019801980197)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'films', u'sci-fi', 0.5397111913357401), (u'films', u'successful', 0.6094822669567604), (u'films', u'various', 0.616617881852049), (u'encounters', u'close', 0.5761380779128582), (u'encounters', u'such', 0.5779893475366177), (u'kind', u'third', 0.5261104174099176), (u'hell', u'sure', -0.5380680511916621), (u'attempt', u'shoddy', -0.9285714285714285), (u'results', u'disastrous', -0.5371747211895918), (u'bit', u'little', 0.5325569647518579), (u'everything', u'little', 0.5454821414598212), (u'energy', u'unknown', 0.5103194544149318), (u'force', u'unknown', 0.5853967780429593), (u'gang', u'previous', 0.5328929630628202), (u'space', u'previous', 0.5610429447852762), (u'planet', u'red', -0.568404361856014), (u'way', u'best', 0.5926089148469893), (u'minutes', u'first', -0.5465775758758116), (u'minutes', u'decent', -0.6890391791044774), (u'movie', u'entire', 0.5256882512601785), (u'space', u'more', 0.5048564518645654), (u'planet', u'cherry-colored', -0.578740157480315), (u'movie', u'sci-fi', -0.5244372990353697), (u'secret', u'big', 0.5041692000347434), (u'secret', u'big', 0.5041692000347434), (u'premise', u'decent', -0.6612636869502783), (u'crux', u'main', 0.7522590361445785), (u'endings', u'anti-climactic', -0.5), (u'side', u'anti-climactic', 0.5918727915194346), (u'ditty', u'frivolous', -0.8333333333333331), (u'computer', u'distracting', 0.6551600401894645), (u'effects', u'distracting', 0.6127122124863088), (u'dialogue', u'bad', -0.7358205925502171), (u'lines', u'cheezy', -0.618951612903226), (u'nature', u'derivative', -0.6554524361948957), (u'nature', u'obvious', 0.5316491819318367), (u'crew', u'fine', 0.5765110941086458), (u'thespians', u'fine', 0.5729166666666669), (u'score', u'musical', 0.7167119565217394), (u'score', u\"film's\", 0.6139965502259861), (u'minutes', u'last', -0.5716504527066075), (u'scene', u'brian', -0.5220585179335853), (u'movie', u'salvageable', -0.8464812123728463), (u'job', u'great', 0.6745633556540227), (u'product', u'devious', -0.9337944664031621), (u'substance', u'actual', -0.6062124248496994), (u'substance', u'little', -0.6255082678232586), (u'dialogue', u'two-bit', -0.5841708542713567), (u'mumbo-jumbo', u'sci-fi', -0.5), (u'computer', u'pathetic', -0.6320347779200652), (u'part', u'graphic', 0.6328804671672029), (u'dreck', u'rehashed', -0.9540816326530612), (u'gate', u'ninth', -0.6941747572815535), (u'movie', u'great', 0.5656791121768954), (u'note', u'personal', 0.6773380195599021), (u'time', u\"it's\", 0.5095313741064336), (u'depalma', u\"it's\", -0.7916666666666666), (u'film', u'uninterrupted', -0.7411306605134123), (u'film', u'12-minute', 0.5116875712656785), (u'film', u'proverbial', -0.7411306605134119), (u'sequences', u'uninterrupted', -0.7911153119092628), (u'sequences', u'12-minute', -0.5579999999999999), (u'movies', u'crappy', -0.8747024821489289)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'grant', u'alan', -0.5089779005524859), (u'park', u'jurassic', 0.6220379146919429), (u')', u'jurassic', 0.5769230769230772), (u'science', u'sexy', -0.5312269031781232), (u'matter', u'subject', 0.6425277561965866), (u'adventures', u'interested', 0.5809859154929574), (u'site', u'notorious', -0.5080321285140557), (u'b', u'notorious', -0.5227272727272729), (u'world', u'lost', 0.605778199936297), (u'agenda', u'real', 0.6349065420560746), (u'park', u'jurassic', 0.6220379146919429), (u'(', u'eric', -0.6020408163265305), (u'trevor', u'eric', -0.6792763157894737), (u'morgan', u'eric', -0.5726361928616157), (u'boy', u'young', 0.6090181622010427), (u'adventure', u'paragliding', -0.6720779220779225), (u'island', u'same', -0.537841842089018), (u'(', u'eric', -0.6020408163265305), (u'projection', u'rear', 0.5188679245283009), (u'projection', u'cheesy', -0.5866935483870969), (u'hand', u'right', 0.5764141344699574), (u'hand', u'new', 0.5604269022223354), (u'man', u'right', 0.5818866245147158), (u'man', u'new', 0.5659504056368164), (u'(', u'brennan', -0.75), (u'flame', u'old', -0.5128737541528235), (u'dr', u'old', 0.5149253731343284), (u'park', u'jurassic', 0.6220379146919429), (u')', u'jurassic', 0.5769230769230772), (u'son', u'young', 0.6515327156210455), (u'purpose', u'sole', -0.6888573543928922), (u'gambit', u'illegal', -0.5942622950819672), (u'paraglider', u'young', -0.6860341151385931), (u\"they're\", u'bogus', -0.5759430427179617), (u'boyfriend', u'new', 0.5031792614331135), (u'boyfriend', u\"amanda's\", -0.7321428571428569), (u'sense', u'much', 0.5459210924277818), (u')', u'much', 0.5043898156277435), (u'family', u'gooey', -0.612157534246575), (u'dynamics', u'gooey', -0.6428571428571429), (u'bait', u'obvious', -0.710624417520969), (u')', u'obvious', -0.595703125), (u')', u'obvious', -0.595703125), (u'script', u'risible', -0.7292115467470913), (u'peter', u'risible', -0.6200265251989391), (u'park', u'jurassic', 0.6220379146919429), (u'monster', u'quickie', 0.6499999999999997), (u'flick', u'quickie', 0.6601362862010223), (u'dinos', u'new', -0.6702724358974359), (u'leaps', u'extreme', 0.6307733619763692), (u'faith', u'extreme', 0.759375), (u'cell', u'stupid', -0.7899929111531191), (u'phone', u'stupid', -0.8095834940223681), (u'tricks', u'stupid', -0.7406668383110196), (u'times', u'murky', -0.6751112386967129), (u'reason', u'other', -0.5952093746780156), (u'ending', u'ridiculous', -0.7730753311258279), (u'logic', u'ludicrous', -0.942488262910798), (u'minutes', u'few', -0.5604996666111017), (u'themes', u'original', 0.7275628179435794), (u'attempt', u'morgan', -0.6219135802469133), (u'attempt', u'young', -0.5749484536082474), (u'park', u'jurassic', 0.6220379146919429), (u'entertainment', u'quick', 0.5583438685208595), (u'crowd', u'same', 0.5211540992235819), (u'world', u'lost', 0.605778199936297), (u'park', u'jurassic', 0.6220379146919429)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob\n",
      "from textblob.sentiments import NaiveBayesAnalyzer\n",
      "\n",
      "text ='eggs'\n",
      "#blob = TextBlob(''.join(text))\n",
      "#print blob\n",
      "#for sentence in blob.sentences:\n",
      "   #print sentence.sentiment.polarity\n",
      "    \n",
      "text =''\n",
      "#blob = TextBlob(''.join(text))\n",
      "#print blob\n",
      "#for sentence in blob.sentences:\n",
      "   #print sentence.sentiment.polarity\n",
      "    \n",
      "near=\"good\"\n",
      "string=text+' '+ near\n",
      "print string\n",
      "blob = TextBlob(string, analyzer=NaiveBayesAnalyzer())\n",
      "for nearby in blob.sentences:\n",
      "     dummy=nearby.sentiment.p_pos\n",
      "     if dummy > 0.50 :\n",
      "                                dummy=dummy\n",
      "     else :\n",
      "                                dummy=-1*(float(1-dummy))\n",
      "     print dummy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " good\n",
        "0.504226542688"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"mainchildren.txt\", \"w\")\n",
      "for child in root:\n",
      "   f.write(child.tag)\n",
      "   f.write(str(child.attrib))\n",
      "   f.write(\"\\n\")\n",
      " \n",
      "f.close()\n",
      "#print file(\"mainchildren.txt\").read() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (root.tag)       #{http://schemas.xmlsoap.org/wsdl/}definitions\n",
      "print(root.attrib)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{http://schemas.xmlsoap.org/wsdl/}definitions\n",
        "{'targetNamespace': 'http://www.webservicex.net'}\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#[elem.attrib for elem in root.iter()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"children.txt\", \"w\")\n",
      "for elem in root.iter():\n",
      "   f.write(elem.tag)\n",
      "   f.write(str(elem.attrib))\n",
      "   f.write(\"\\n\")\n",
      " \n",
      "f.close()\n",
      "#print file(\"children.txt\").read() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xyz=[]\n",
      "#root.findall(\"./\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import xml.etree.ElementTree as ET\n",
      "#tree = ET.parse('globalweather.asmx')\n",
      "#root = tree.getroot()\n",
      "'''\n",
      "for child in root:\n",
      "   #print child.tag\n",
      "    \n",
      "#from xml.etree.ElementTree import XMLParser\n",
      " class MaxDepth:                     # The target object of the parser\n",
      "    maxDepth = 0\n",
      "    self=root\n",
      "    depth = 0\n",
      "    def start(self, tag, attrib):   # Called for each opening tag.\n",
      "        self.depth += 1\n",
      "        if self.depth > self.maxDepth:\n",
      "             self.maxDepth = self.depth\n",
      "    def end(self, tag):             # Called for each closing tag.\n",
      "         self.depth -= 1\n",
      "    def data(self, data):\n",
      "        pass            # We do not need to do anything with data.\n",
      "    def close(self):    # Called when all data has been parsed.\n",
      "        return self.maxDepth\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "'\\nfor child in root:\\n   #print child.tag\\n    \\n#from xml.etree.ElementTree import XMLParser\\n class MaxDepth:                     # The target object of the parser\\n    maxDepth = 0\\n    self=root\\n    depth = 0\\n    def start(self, tag, attrib):   # Called for each opening tag.\\n        self.depth += 1\\n        if self.depth > self.maxDepth:\\n             self.maxDepth = self.depth\\n    def end(self, tag):             # Called for each closing tag.\\n         self.depth -= 1\\n    def data(self, data):\\n        pass            # We do not need to do anything with data.\\n    def close(self):    # Called when all data has been parsed.\\n        return self.maxDepth\\n'"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FC = open(\"WeatherForecast.asmx\", \"r\")\n",
      "FCC = open(\"children.txt\", \"r\")\n",
      "FF = open(\"content.txt\", \"w+b\")\n",
      "FM = open(\"message.txt\", \"w+b\")\n",
      "FT = open(\"types.txt\", \"w+b\")\n",
      "\n",
      "\n",
      "elements=FC.read()#needed for newline(message)\n",
      "#print elements\n",
      "elementsc=FCC.read()#for content and message(wsdl)\n",
      "#print elementsc\n",
      "import re\n",
      "import pandas as pd\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "string=re.sub(r\"{http.*/}\",\"\",elementsc,flags=re.I)\n",
      "#print string\n",
      "string2=re.sub(r\"{http[^{}]*}\",\"\",string,flags=re.I)\n",
      "#print string2\n",
      "FF.write(string2)\n",
      "#ff.write(elementsc)\n",
      "\n",
      "\n",
      "newline=re.sub(r'\\n',\"\",elements,flags=re.I)\n",
      "#print newline;\n",
      "newline1=re.sub(r'\\n',\"\",string2,flags=re.I)\n",
      "#print newline1;\n",
      "\n",
      "'''matchObj=re.findall(r\"message{.*\",string,flags=re.I)\n",
      "if matchObj:\n",
      "   print matchObj\n",
      "   for item in matchObj:\n",
      "     fm.write(\"%s\\n\" % item)\n",
      "else:\n",
      "   print \"No match for message\"'''\n",
      "    \n",
      "\n",
      "matchObj=re.findall(r\"message{(.*?)}\",string2,flags=re.I)\n",
      "if matchObj:\n",
      "   #print matchObj\n",
      "   for item in matchObj:\n",
      "         FM.write(\"%s\\n\" % item)\n",
      "   matchObj2=re.findall(r\"part{(.*?)}\",string2,flags=re.I)\n",
      "   if matchObj2:\n",
      "       #print matchObj2\n",
      "       for item in matchObj:\n",
      "         FM.write(\"%s\\n\" % item)\n",
      "else:\n",
      "   matchObj=re.findall(r\"<message(.*?)[^>](.*?)</message>\",newline,flags=re.I)\n",
      "   if matchObj:\n",
      "       #print matchObj\n",
      "       for item in matchObj:\n",
      "        for items in item:\n",
      "         FM.write(\"%s\\n\" % items)\n",
      "   else:\n",
      "       print \"No match at all for message\"\n",
      "\n",
      "                \n",
      "matchObj=re.findall(r\"complexType{(.*?)}\",string2,flags=re.I)\n",
      "if matchObj:\n",
      "   print matchObj\n",
      "   for item in matchObj:\n",
      "         FT.write(\"%s\\n\" % item)\n",
      "   matchObj2=re.findall(r\"element{(.*?)}\",string2,flags=re.I)\n",
      "   if matchObj2:\n",
      "       print matchObj2\n",
      "       for item in matchObj:\n",
      "         FT.write(\"%s\\n\" % item)\n",
      "else:\n",
      "   print \"No match at all for type\"\n",
      "\n",
      "FC.close()\n",
      "FCC.close()\n",
      "FT.close()\n",
      "FM.close()\n",
      "FF.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['', '', \"'name': 'WeatherForecasts'\", \"'name': 'ArrayOfWeatherData'\", \"'name': 'WeatherData'\", '', '']\n",
        "[\"'name': 'GetWeatherByZipCode'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'ZipCode', 'minOccurs': '0'\", \"'name': 'GetWeatherByZipCodeResponse'\", \"'maxOccurs': '1', 'type': 'tns:WeatherForecasts', 'name': 'GetWeatherByZipCodeResult', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:float', 'name': 'Latitude', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:float', 'name': 'Longitude', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:float', 'name': 'AllocationFactor', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'FipsCode', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'PlaceName', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'StateCode', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'Status', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 'tns:ArrayOfWeatherData', 'name': 'Details', 'minOccurs': '0'\", \"'maxOccurs': 'unbounded', 'type': 'tns:WeatherData', 'name': 'WeatherData', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'Day', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'WeatherImage', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MaxTemperatureF', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MinTemperatureF', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MaxTemperatureC', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MinTemperatureC', 'minOccurs': '0'\", \"'name': 'GetWeatherByPlaceName'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'PlaceName', 'minOccurs': '0'\", \"'name': 'GetWeatherByPlaceNameResponse'\", \"'maxOccurs': '1', 'type': 'tns:WeatherForecasts', 'name': 'GetWeatherByPlaceNameResult', 'minOccurs': '1'\", \"'type': 'tns:WeatherForecasts', 'name': 'WeatherForecasts'\"]\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from SOAPpy import WSDL     \n",
      "from suds.client import Client\n",
      "import re\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "fp = open(\"portType.txt\", \"w+b\")\n",
      "fs = open(\"services.txt\", \"w+b\")\n",
      "fs1 = open(\"sudsps.txt\", \"w\")\n",
      "\n",
      "url = 'file:///home/swathi/WeatherForecast.asmx'\n",
      "c = Client(url)\n",
      "#print c;\n",
      "fs1.write(\"%s\\n\" % c)\n",
      "\n",
      "fs1.close()\n",
      "\n",
      "fs2 = open(\"sudsps.txt\", \"r\")\n",
      "sudselements=fs2.read()\n",
      "#print sudselements\n",
      "\n",
      "newline2=re.sub(r'\\n',\"\",sudselements,flags=re.I)\n",
      "#print newline1;\n",
      "\n",
      "matchObj=re.findall(r\"Ports(.*)\",newline2,flags=re.I)\n",
      "if matchObj:\n",
      "   #print matchObj\n",
      "   for item in matchObj:\n",
      "        for items in item:\n",
      "         fp.write(items)\n",
      "else:\n",
      "   print \"No match for ports\"\n",
      "    \n",
      "matchObj=re.findall(r\"Service(.*)\\((.*)\\)\",sudselements,flags=re.I)\n",
      "if matchObj:\n",
      "   #print matchObj\n",
      "   for item in matchObj:\n",
      "        for items in item:\n",
      "         fs.write(\"%s\\n\" % items)\n",
      "else:\n",
      "   print \"No match for service name\"\n",
      "\n",
      "\n",
      "fp.close()\n",
      "fs2.close()\n",
      "fs.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmed_filem = open(\"M.txt\",\"w+b\")\n",
      "stemmed_files = open(\"S.txt\",\"w+b\")\n",
      "stemmed_filep = open(\"P.txt\",\"w+b\")\n",
      "stemmed_filec = open(\"C.txt\",\"w+b\")\n",
      "stemmed_filet = open(\"T.txt\", \"w+b\")\n",
      "\n",
      "import re\n",
      "from nltk import PorterStemmer\n",
      "with open('portType.txt','r') as f:\n",
      "    for line in f:\n",
      "      for word in line.split():\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       new=stemmer.stem(all_alpha)\n",
      "       #print new\n",
      "       stemmed_filep.write(new)\n",
      "\n",
      "with open('message.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       new=stemmer.stem(all_alpha)\n",
      "       stemmed_filem.write(new)\n",
      "\n",
      "with open('services.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       stemmer.stem(all_alpha)\n",
      "       stemmed_files.write(all_alpha)\n",
      "\n",
      "with open('types.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       stemmer.stem(all_alpha)\n",
      "       stemmed_filet.write(all_alpha)\n",
      "\n",
      "with open('content.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub(r'^\\'https?:\\/\\/.*[\\r\\n]*\\'','',word, flags=re.I)\n",
      "       all_alpha = re.sub(r'^\\{http?:\\/\\/.*[\\r\\n]*\\\\}','',all_alpha, flags=re.I)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',all_alpha, flags=re.I)\n",
      "       #print all_alpha\n",
      "       for word2 in all_alpha.split():\n",
      "           #print word2\n",
      "           stemmer=PorterStemmer()\n",
      "           new=stemmer.stem(word2)\n",
      "           #print new\n",
      "           stemmed_filec.write(new)\n",
      "           stemmed_filec.write(' ')\n",
      "\n",
      "stemmed_filec.close()\n",
      "stemmed_filem.close()\n",
      "stemmed_files.close()\n",
      "stemmed_filet.close()\n",
      "stemmed_filep.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import PorterStemmer\n",
      "stemmer=PorterStemmer()\n",
      "word='execution'\n",
      "stemmer.stem(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "u'execut'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''import difflib as dl\n",
      "\n",
      "a = file('WS/1/C.txt').read()\n",
      "b = file('WS/2/C.txt').read()\n",
      "\n",
      "sim = dl.get_close_matches\n",
      "\n",
      "s = 0\n",
      "wa = a.split()\n",
      "wb = b.split()\n",
      "\n",
      "for i in wa:\n",
      "    if sim(i, wb):\n",
      "        s += 1\n",
      "\n",
      "n = float(s) / float(len(wa))\n",
      "print '%d%% similarity' % int(n * 100)'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 151,
       "text": [
        "\"import difflib as dl\\n\\na = file('WS/1/C.txt').read()\\nb = file('WS/2/C.txt').read()\\n\\nsim = dl.get_close_matches\\n\\ns = 0\\nwa = a.split()\\nwb = b.split()\\n\\nfor i in wa:\\n    if sim(i, wb):\\n        s += 1\\n\\nn = float(s) / float(len(wa))\\nprint '%d%% similarity' % int(n * 100)\""
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=0\n",
      "F = open(\"WS/N.txt\",\"w+b\")\n",
      "\n",
      "with open(\"WS/6/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "     #    with open(\"WS/N.txt\",\"r\") as f2:\n",
      "     #       for line2 in f:\n",
      "      #          for word2 in line2.split():\n",
      "     #             if word == word2\n",
      "     #               Na = f2.read(word2)\n",
      "      #              F.write(str(Na+1)+'\\n')\n",
      "    #                break\n",
      "    #              else\n",
      "    #               continue\n",
      "    #            if(word!=word2)\n",
      "    #                Na=1\n",
      "   #                 F.write(word)#if not present, add the word with count equal to 1\n",
      "   #                 F.write(\" \")\n",
      "   #                 F.write(str(Na)+'\\n')                   \n",
      "          #F.write(all_alpha)'''\n",
      "        \n",
      "with open(\"WS/4/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "\n",
      "with open(\"WS/0/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "                \n",
      "with open(\"WS/2/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "          \n",
      "with open(\"WS/3/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "                \n",
      "with open(\"WS/7/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "F.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "F = open(\"WS/C.txt\",\"w+b\")\n",
      "LamAv=0\n",
      "print N\n",
      "with open(\"WS/5/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract word\n",
      "            if(len(word)>1):\n",
      "              a=word\n",
      "              Fa=1\n",
      "              Nw=1\n",
      "              with open(\"WS/5/C.txt\",\"r\") as f2:\n",
      "               for line2 in f2:\n",
      "                for word2 in line2.split():\n",
      "                    if (a==word2):\n",
      "                     Fa=Fa+1\n",
      "              with open(\"WS/3/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/4/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/7/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/2/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/0/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "\n",
      "              #print Fa\n",
      "              #print Nw\n",
      "              La=float(Fa*100/N)\n",
      "              #print La\n",
      "              temp=(np.random.poisson(0,La))\n",
      "              Na = N*(1-temp)\n",
      "              Lam=Na/Nw\n",
      "              Lamthr= Nw/5#no of docs observed\n",
      "              if (Lam.all() > Lamthr):\n",
      "                    F.write(a+'\\n')\n",
      "                    \n",
      "F.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "22424\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/5/NonDup2.txt\", \"w\")\n",
      "for line in open(\"WS/C.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"WS/5/NonDup.txt\", \"w\")\n",
      "for line in open(\"WS/C.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "#outfile = open(\"WS/3/T1.txt\", \"w\")\n",
      "#for line in open(\"WS/3/T.txt\", \"r\"):\n",
      "#  for word in line.split():\n",
      "#    if(word!=\"name\"):\n",
      "#        outfile.write(word+'\\n')\n",
      "#outfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "outfile = open(\"WS/5/T1.txt\", \"w\")\n",
      "for line in open(\"WS/5/T.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "outfile = open(\"WS/1/T1.txt\", \"w\")\n",
      "for line in open(\"WS/1/T.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/5/T2.txt\", \"w\")\n",
      "for line in open(\"WS/5/T1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/1/T2.txt\", \"w\")\n",
      "for line in open(\"WS/1/T1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "Mt=0\n",
      "F = open(\"WS/5/T2.txt\",\"r\")\n",
      "for line in F:\n",
      "  for word in line.split():\n",
      "   a=word\n",
      "   print word \n",
      "   with open(\"WS/1/T2.txt\",\"r\") as f:\n",
      "    for line2 in f:\n",
      "        for word2 in line2.split():#extract word\n",
      "             if(a==word2) :\n",
      "              Mt=Mt+1\n",
      "print Mt\n",
      "\n",
      "F.close()\n",
      "\n",
      "E1=0\n",
      "E2=0\n",
      "\n",
      "F1 = open(\"WS/5/T2.txt\",\"r\")\n",
      "for line in F1:\n",
      "  for word in line.split():\n",
      "   E1=E1+1\n",
      "F1.close()\n",
      "\n",
      "F2 = open(\"WS/1/T2.txt\",\"r\")\n",
      "for line in F2:\n",
      "  for word in line.split():\n",
      "   E2=E2+1\n",
      "F2.close()\n",
      "from decimal import Decimal\n",
      "print E1\n",
      "print E2\n",
      "print Decimal(E1+E2)/2\n",
      "print Decimal(Mt/(Decimal(E1+E2)/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mixed\n",
        "true\n",
        "0\n",
        "2\n",
        "3\n",
        "2.5\n",
        "0E+1\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"WS/5/M1.txt\", \"w\")\n",
      "for line in open(\"WS/5/M.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "outfile = open(\"WS/1/M1.txt\", \"w\")\n",
      "for line in open(\"WS/1/M.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()                \n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/5/M2.txt\", \"w\")\n",
      "for line in open(\"WS/5/M1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/1/M2.txt\", \"w\")\n",
      "for line in open(\"WS/1/M1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "Mt=0\n",
      "F = open(\"WS/5/M2.txt\",\"r\")\n",
      "for line in F:\n",
      "  for word in line.split():\n",
      "   a=word\n",
      "  # print word \n",
      "   with open(\"WS/1/M2.txt\",\"r\") as f:\n",
      "    for line2 in f:\n",
      "        for word2 in line2.split():#extract word\n",
      "             if(a==word2) :\n",
      "              Mt=Mt+1\n",
      "print \"Matching words -\",Mt\n",
      "F.close()\n",
      "\n",
      "E1=0\n",
      "E2=0\n",
      "\n",
      "F1 = open(\"WS/5/M2.txt\",\"r\")\n",
      "for line in F1:\n",
      "  for word in line.split():\n",
      "   E1=E1+1\n",
      "F1.close()\n",
      "\n",
      "F2 = open(\"WS/1/M2.txt\",\"r\")\n",
      "for line in F2:\n",
      "  for word in line.split():\n",
      "   E2=E2+1\n",
      "F2.close()\n",
      "\n",
      "print \"Number of elements in first document - \",E1\n",
      "print \"Number of elements in second document - \",E2\n",
      "from decimal import Decimal\n",
      "#print Decimal(E1+E2)/2\n",
      "print \"Match between the Messages of two documents - \",Decimal(Mt/(Decimal(E1+E2)/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Matching words - 0\n",
        "Number of elements in first document -  43\n",
        "Number of elements in second document -  12\n",
        "Match between the Messages of two documents -  0E+1\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"WS/5/P1.txt\", \"w\")\n",
      "for line in open(\"WS/5/P.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "outfile = open(\"WS/1/P1.txt\", \"w\")\n",
      "for line in open(\"WS/1/P.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()                \n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/5/P2.txt\", \"w\")\n",
      "for line in open(\"WS/5/P1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/1/P2.txt\", \"w\")\n",
      "for line in open(\"WS/1/P1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "Mt=0\n",
      "F = open(\"WS/5/P2.txt\",\"r\")\n",
      "for line in F:\n",
      "  for word in line.split():\n",
      "   a=word\n",
      "   #print word \n",
      "   with open(\"WS/1/P2.txt\",\"r\") as f:\n",
      "    for line2 in f:\n",
      "        for word2 in line2.split():#extract word\n",
      "             if(a==word2) :\n",
      "              Mt=Mt+1\n",
      "print \"Matching words -\",Mt\n",
      "F.close()\n",
      "\n",
      "E1=0\n",
      "E2=0\n",
      "\n",
      "F1 = open(\"WS/5/P2.txt\",\"r\")\n",
      "for line in F1:\n",
      "  for word in line.split():\n",
      "   E1=E1+1\n",
      "F1.close()\n",
      "\n",
      "F2 = open(\"WS/1/P2.txt\",\"r\")\n",
      "for line in F2:\n",
      "  for word in line.split():\n",
      "   E2=E2+1\n",
      "F2.close()\n",
      "\n",
      "print \"Number of elements in first document - \",E1\n",
      "print \"Number of elements in second document - \",E2\n",
      "from decimal import Decimal\n",
      "#print Decimal(E1+E2)/2\n",
      "print \"Match between the Ports of two documents - \",Decimal(Mt/(Decimal(E1+E2)/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Matching words - 3\n",
        "Number of elements in first document -  31\n",
        "Number of elements in second document -  9\n",
        "Match between the Ports of two documents -  0.15\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "F = open(\"Practice.txt\",\"w+b\")\n",
      "F.write(\"hello\")\n",
      "N=2\n",
      "F.write(\" \")\n",
      "F.write(str(N)+'\\n')\n",
      "F.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word=\"hello\"\n",
      "word2=\"hello\"\n",
      "\n",
      "len(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "s = np.random.poisson(0, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn import datasets\n",
      "\n",
      "np.random.seed(5)\n",
      "\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n",
      "              'k_means_iris_8': KMeans(n_clusters=8),\n",
      "              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n",
      "                                              init='random')}\n",
      "\n",
      "\n",
      "fignum = 1\n",
      "for name, est in estimators.items():\n",
      "    fig = plt.figure(fignum, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "    plt.cla()\n",
      "    est.fit(X)\n",
      "    labels = est.labels_\n",
      "\n",
      "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
      "\n",
      "    ax.w_xaxis.set_ticklabels([])\n",
      "    ax.w_yaxis.set_ticklabels([])\n",
      "    ax.w_zaxis.set_ticklabels([])\n",
      "    ax.set_xlabel('Petal width')\n",
      "    ax.set_ylabel('Sepal length')\n",
      "    ax.set_zlabel('Petal length')\n",
      "    fignum = fignum + 1\n",
      "\n",
      "# Plot the ground truth\n",
      "fig = plt.figure(fignum, figsize=(4, 3))\n",
      "plt.clf()\n",
      "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "plt.cla()\n",
      "\n",
      "for name, label in [('Setosa', 0),\n",
      "                    ('Versicolour', 1),\n",
      "                    ('Virginica', 2)]:\n",
      "    ax.text3D(X[y == label, 3].mean(),\n",
      "              X[y == label, 0].mean() + 1.5,\n",
      "              X[y == label, 2].mean(), name,\n",
      "              horizontalalignment='center',\n",
      "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
      "# Reorder the labels to have colors matching the cluster results\n",
      "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
      "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
      "\n",
      "ax.w_xaxis.set_ticklabels([])\n",
      "ax.w_yaxis.set_ticklabels([])\n",
      "ax.w_zaxis.set_ticklabels([])\n",
      "ax.set_xlabel('Petal width')\n",
      "ax.set_ylabel('Sepal length')\n",
      "ax.set_zlabel('Petal length')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/swathi/anaconda/lib/python2.7/site-packages/mpl_toolkits/mplot3d/axes3d.py:1094: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  if self.button_pressed in self._rotate_btn:\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn import datasets\n",
      "\n",
      "np.random.seed(5)\n",
      "\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n",
      "              'k_means_iris_8': KMeans(n_clusters=8),\n",
      "              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n",
      "                                              init='random')}\n",
      "\n",
      "\n",
      "fignum = 1\n",
      "for name, est in estimators.items():\n",
      "    fig = plt.figure(fignum, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "    plt.cla()\n",
      "    est.fit(X)\n",
      "    labels = est.labels_\n",
      "\n",
      "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
      "\n",
      "    ax.w_xaxis.set_ticklabels([])\n",
      "    ax.w_yaxis.set_ticklabels([])\n",
      "    ax.w_zaxis.set_ticklabels([])\n",
      "    ax.set_xlabel('Petal width')\n",
      "    ax.set_ylabel('Sepal length')\n",
      "    ax.set_zlabel('Petal length')\n",
      "    fignum = fignum + 1\n",
      "\n",
      "# Plot the ground truth\n",
      "fig = plt.figure(fignum, figsize=(4, 3))\n",
      "plt.clf()\n",
      "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "plt.cla()\n",
      "\n",
      "for name, label in [('Setosa', 0),\n",
      "                    ('Versicolour', 1),\n",
      "                    ('Virginica', 2)]:\n",
      "    ax.text3D(X[y == label, 3].mean(),\n",
      "              X[y == label, 0].mean() + 1.5,\n",
      "              X[y == label, 2].mean(), name,\n",
      "              horizontalalignment='center',\n",
      "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
      "# Reorder the labels to have colors matching the cluster results\n",
      "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
      "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
      "\n",
      "ax.w_xaxis.set_ticklabels([])\n",
      "ax.w_yaxis.set_ticklabels([])\n",
      "ax.w_zaxis.set_ticklabels([])\n",
      "ax.set_xlabel('Petal width')\n",
      "ax.set_ylabel('Sepal length')\n",
      "ax.set_zlabel('Petal length')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import cluster\n",
      "\n",
      "n_clusters = 5\n",
      "np.random.seed(0)\n",
      "\n",
      "try:\n",
      "    lena = sp.lena()\n",
      "except AttributeError:\n",
      "    # Newer versions of scipy have lena in misc\n",
      "    from scipy import misc\n",
      "    lena = misc.lena()\n",
      "X = lena.reshape((-1, 1))  # We need an (n_sample, n_feature) array\n",
      "k_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\n",
      "k_means.fit(X)\n",
      "values = k_means.cluster_centers_.squeeze()\n",
      "labels = k_means.labels_\n",
      "\n",
      "# create an array from labels and values\n",
      "lena_compressed = np.choose(labels, values)\n",
      "lena_compressed.shape = lena.shape\n",
      "\n",
      "vmin = lena.min()\n",
      "vmax = lena.max()\n",
      "\n",
      "# original lena\n",
      "plt.figure(1, figsize=(3, 2.2))\n",
      "plt.imshow(lena, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
      "\n",
      "# compressed lena\n",
      "plt.figure(2, figsize=(3, 2.2))\n",
      "plt.imshow(lena_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# equal bins lena\n",
      "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
      "regular_labels = np.searchsorted(regular_values, lena) - 1\n",
      "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
      "regular_lena = np.choose(regular_labels.ravel(), regular_values)\n",
      "regular_lena.shape = lena.shape\n",
      "plt.figure(3, figsize=(3, 2.2))\n",
      "plt.imshow(regular_lena, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# histogram\n",
      "plt.figure(4, figsize=(3, 2.2))\n",
      "plt.clf()\n",
      "plt.axes([.01, .01, .98, .98])\n",
      "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
      "plt.yticks(())\n",
      "plt.xticks(regular_values)\n",
      "values = np.sort(values)\n",
      "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
      "\n",
      "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import cluster\n",
      "\n",
      "n_clusters = 5\n",
      "np.random.seed(0)\n",
      "\n",
      "try:\n",
      "    lena = sp.lena()\n",
      "except AttributeError:\n",
      "    # Newer versions of scipy have lena in misc\n",
      "    from scipy import misc\n",
      "    lena = misc.lena()\n",
      "X = lena.reshape((-1, 1))  # We need an (n_sample, n_feature) array\n",
      "k_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\n",
      "k_means.fit(X)\n",
      "values = k_means.cluster_centers_.squeeze()\n",
      "labels = k_means.labels_\n",
      "\n",
      "# create an array from labels and values\n",
      "lena_compressed = np.choose(labels, values)\n",
      "lena_compressed.shape = lena.shape\n",
      "\n",
      "vmin = lena.min()\n",
      "vmax = lena.max()\n",
      "\n",
      "# original lena\n",
      "plt.figure(1, figsize=(3, 2.2))\n",
      "plt.imshow(lena, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
      "\n",
      "# compressed lena\n",
      "plt.figure(2, figsize=(3, 2.2))\n",
      "plt.imshow(lena_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# equal bins lena\n",
      "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
      "regular_labels = np.searchsorted(regular_values, lena) - 1\n",
      "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
      "regular_lena = np.choose(regular_labels.ravel(), regular_values)\n",
      "regular_lena.shape = lena.shape\n",
      "plt.figure(3, figsize=(3, 2.2))\n",
      "plt.imshow(regular_lena, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# histogram\n",
      "plt.figure(4, figsize=(3, 2.2))\n",
      "plt.clf()\n",
      "plt.axes([.01, .01, .98, .98])\n",
      "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
      "plt.yticks(())\n",
      "plt.xticks(regular_values)\n",
      "values = np.sort(values)\n",
      "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
      "\n",
      "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn import datasets\n",
      "\n",
      "np.random.seed(5)\n",
      "\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n",
      "              'k_means_iris_8': KMeans(n_clusters=8),\n",
      "              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n",
      "                                              init='random')}\n",
      "\n",
      "\n",
      "fignum = 1\n",
      "for name, est in estimators.items():\n",
      "    fig = plt.figure(fignum, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "    plt.cla()\n",
      "    est.fit(X)\n",
      "    labels = est.labels_\n",
      "\n",
      "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
      "\n",
      "    ax.w_xaxis.set_ticklabels([])\n",
      "    ax.w_yaxis.set_ticklabels([])\n",
      "    ax.w_zaxis.set_ticklabels([])\n",
      "    ax.set_xlabel('Petal width')\n",
      "    ax.set_ylabel('Sepal length')\n",
      "    ax.set_zlabel('Petal length')\n",
      "    fignum = fignum + 1\n",
      "\n",
      "# Plot the ground truth\n",
      "fig = plt.figure(fignum, figsize=(4, 3))\n",
      "plt.clf()\n",
      "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "plt.cla()\n",
      "\n",
      "for name, label in [('Setosa', 0),\n",
      "                    ('Versicolour', 1),\n",
      "                    ('Virginica', 2)]:\n",
      "    ax.text3D(X[y == label, 3].mean(),\n",
      "              X[y == label, 0].mean() + 1.5,\n",
      "              X[y == label, 2].mean(), name,\n",
      "              horizontalalignment='center',\n",
      "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
      "# Reorder the labels to have colors matching the cluster results\n",
      "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
      "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
      "\n",
      "ax.w_xaxis.set_ticklabels([])\n",
      "ax.w_yaxis.set_ticklabels([])\n",
      "ax.w_zaxis.set_ticklabels([])\n",
      "ax.set_xlabel('Petal width')\n",
      "ax.set_ylabel('Sepal length')\n",
      "ax.set_zlabel('Petal length')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/swathi/anaconda/lib/python2.7/site-packages/mpl_toolkits/mplot3d/axes3d.py:1094: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  if self.button_pressed in self._rotate_btn:\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "X = [[10, 20,30,1],[30,40,5,7],[2,4,5,8],[1,2,3,4]]\n",
      "\n",
      "listofwords = ['physics', 'chemistry', 'maths', 'bio'];\n",
      "\n",
      "\n",
      "\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(X)\n",
      "    \n",
      "prediction = km.predict(lst)\n",
      "\n",
      "print('{:<15}\\t{}'.format('Word','Cluster'))\n",
      "for i in range(len(prediction)):\n",
      "    print('{:<15}\\t{}'.format(listofwords[i],prediction[i]+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "\n",
      "import logging\n",
      "from optparse import OptionParser\n",
      "import sys\n",
      "from time import time\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "# Display progress logs on stdout\n",
      "logging.basicConfig(level=logging.INFO,\n",
      "                    format='%(asctime)s %(levelname)s %(message)s')\n",
      "\n",
      "# parse commandline arguments\n",
      "op = OptionParser()\n",
      "op.add_option(\"--lsa\",\n",
      "              dest=\"n_components\", type=\"int\",\n",
      "              help=\"Preprocess documents with latent semantic analysis.\")\n",
      "op.add_option(\"--no-minibatch\",\n",
      "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
      "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
      "op.add_option(\"--no-idf\",\n",
      "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
      "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
      "op.add_option(\"--use-hashing\",\n",
      "              action=\"store_true\", default=False,\n",
      "              help=\"Use a hashing feature vectorizer\")\n",
      "op.add_option(\"--n-features\", type=int, default=10000,\n",
      "              help=\"Maximum number of features (dimensions)\"\n",
      "                   \" to extract from text.\")\n",
      "op.add_option(\"--verbose\",\n",
      "              action=\"store_true\", dest=\"verbose\", default=False,\n",
      "              help=\"Print progress reports inside k-means algorithm.\")\n",
      "\n",
      "print(__doc__)\n",
      "op.print_help()\n",
      "\n",
      "(opts, args) = op.parse_args()\n",
      "if len(args) > 0:\n",
      "    op.error(\"this script takes no arguments.\")\n",
      "    sys.exit(1)\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Load some categories from the training set\n",
      "categories = [\n",
      "    'alt.atheism',\n",
      "    'talk.religion.misc',\n",
      "    'comp.graphics',\n",
      "    'sci.space',\n",
      "]\n",
      "# Uncomment the following to do the analysis on all the categories\n",
      "#categories = None\n",
      "\n",
      "print(\"Loading 20 newsgroups dataset for categories:\")\n",
      "print(categories)\n",
      "\n",
      "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
      "                             shuffle=True, random_state=42)\n",
      "\n",
      "print(\"%d documents\" % len(dataset.data))\n",
      "print(\"%d categories\" % len(dataset.target_names))\n",
      "print()\n",
      "\n",
      "labels = dataset.target\n",
      "true_k = np.unique(labels).shape[0]\n",
      "\n",
      "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
      "t0 = time()\n",
      "if opts.use_hashing:\n",
      "    if opts.use_idf:\n",
      "        # Perform an IDF normalization on the output of HashingVectorizer\n",
      "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
      "                                   stop_words='english', non_negative=True,\n",
      "                                   norm=None, binary=False)\n",
      "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
      "    else:\n",
      "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
      "                                       stop_words='english',\n",
      "                                       non_negative=False, norm='l2',\n",
      "                                       binary=False)\n",
      "else:\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
      "                                 min_df=2, stop_words='english',\n",
      "                                 use_idf=opts.use_idf)\n",
      "X = vectorizer.fit_transform(dataset.data)\n",
      "\n",
      "print(\"done in %fs\" % (time() - t0))\n",
      "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
      "print()\n",
      "\n",
      "if opts.n_components:\n",
      "    print(\"Performing dimensionality reduction using LSA\")\n",
      "    t0 = time()\n",
      "    # Vectorizer results are normalized, which makes KMeans behave as\n",
      "    # spherical k-means for better results. Since LSA/SVD results are\n",
      "    # not normalized, we have to redo the normalization.\n",
      "    svd = TruncatedSVD(opts.n_components)\n",
      "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
      "\n",
      "    X = lsa.fit_transform(X)\n",
      "\n",
      "    print(\"done in %fs\" % (time() - t0))\n",
      "\n",
      "    explained_variance = svd.explained_variance_ratio_.sum()\n",
      "    print(\"Explained variance of the SVD step: {}%\".format(\n",
      "        int(explained_variance * 100)))\n",
      "\n",
      "    print()\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Do the actual clustering\n",
      "\n",
      "if opts.minibatch:\n",
      "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
      "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
      "else:\n",
      "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
      "                verbose=opts.verbose)\n",
      "\n",
      "print(\"Clustering sparse data with %s\" % km)\n",
      "t0 = time()\n",
      "km.fit(X)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print()\n",
      "\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "print(\"Adjusted Rand-Index: %.3f\"\n",
      "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
      "\n",
      "print()\n",
      "\n",
      "if not (opts.n_components or opts.use_hashing):\n",
      "    print(\"Top terms per cluster:\")\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(true_k):\n",
      "        print(\"Cluster %d:\" % i, end='')\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind], end='')\n",
      "        print()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Automatically created module for IPython interactive environment\n",
        "Usage: -c [options]\n",
        "\n",
        "Options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
        "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
        "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
        "  --use-hashing         Use a hashing feature vectorizer\n",
        "  --n-features=N_FEATURES\n",
        "                        Maximum number of features (dimensions) to extract\n",
        "                        from text.\n",
        "  --verbose             Print progress reports inside k-means algorithm.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Usage: -c [options]\n",
        "\n",
        "-c: error: no such option: -f\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "2",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "To exit: use 'exit', 'quit', or Ctrl-D.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.jet() # set the color map. When your colors are lost, re-run this.\n",
      "import sklearn.datasets as datasets\n",
      "X, Y = datasets.make_blobs(centers=4, cluster_std=0.5, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:,0], X[:,1]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "plt.scatter(X[:,0], X[:,1], c=Y);\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      " X = np.random.randint(5, size=(6, 100))\n",
      " y = np.array([1, 2, 3, 4, 5, 6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "unexpected indent (<ipython-input-9-06cad03e6fbc>, line 2)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-06cad03e6fbc>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    X = np.random.randint(5, size=(6, 100))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X=np.random.randint(5, size=(6,100))\n",
      "y=np.array([1,2,3,4,5,6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X=np.random.randint(5, size=(6,100))\n",
      "y=np.array([1,2,3,4,5,6])\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-11-9a9f73266cf9>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-9a9f73266cf9>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print y\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X = np.random.randint(5, size=(6,100))\n",
      "y = np.array([1,2,3,4,5,6])\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-12-f50deafdc489>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-f50deafdc489>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print y\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "lst = [10, 20, 30, 40]\n",
      "arr = np.array([10, 20, 30, 40])\n",
      "print lst\n",
      "print arr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-13-6e1138b2e5b7>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-6e1138b2e5b7>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print lst\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "#wn.synsets('dog')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/swathi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-25-09296a062901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dog'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/swathi/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/swathi/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/swathi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "lst = [[10, 20,30,1],[30,40,5,7],[2,4,5,8],[1,2,3,4]]\n",
      "#X = np.array([10, 20, 30, 40])\n",
      "\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(lst)\n",
      "\n",
      "\n",
      "centers = km.cluster_centers_\n",
      "centers[centers<0] = 0 #the minimization function may find very small negative numbers, we threshold them to 0\n",
      "#centers = centers.round(2)\n",
      "print('\\n--------Centers of the 2 different clusters--------')\n",
      "print('Deal\\t Cent1\\t Cent2')\n",
      "for i in range(1):\n",
      "    print(i+1,'\\t',centers[0,i],'\\t',centers[1,i])\n",
      "    \n",
      "prediction = km.predict(lst)\n",
      "print('\\n--------Which cluster each customer is in--------')\n",
      "print('{:<15}\\t{}'.format('Customer','Cluster'))\n",
      "for i in range(len(prediction)):\n",
      "    print('{:<15}\\t{}'.format(i,prediction[i]+1))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------Centers of the 2 different clusters--------\n",
        "Deal\t Cent1\t Cent2\n",
        "(1, '\\t', 30.0, '\\t', 4.333333333333333)\n",
        "\n",
        "--------Which cluster each customer is in--------\n",
        "Customer       \tCluster\n",
        "0              \t2\n",
        "1              \t1\n",
        "2              \t2\n",
        "3              \t2\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "lst = [[10, 20,],[30,40]]\n",
      "#arr = np.array([10, 20, 30, 40])\n",
      "print (lst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[10, 20], [30, 40]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "X = [[0,0.5623,0.2663,0.2496],[0.05623,0,0.6456,0.6179],[0.2663,0.6456,0,0.2350],[0.2496,0.6179,0.2350,0]]\n",
      "\n",
      "listofwords = ['weather', 'www', 'climate', 'temperature'];\n",
      "\n",
      "\n",
      "\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(X)\n",
      "    \n",
      "prediction = km.predict(X)\n",
      "\n",
      "print('{:<15}\\t{}'.format('Word','Cluster'))\n",
      "for i in range(len(prediction)):\n",
      "    print('{:<15}\\t{}'.format(listofwords[i],prediction[i]+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word           \tCluster\n",
        "weather        \t2\n",
        "www            \t1\n",
        "climate        \t2\n",
        "temperature    \t2\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X=[[0,1,10,1],[1,0,10,1],[10,10,0,10],[1,1,10,0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}